<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Daniel S. Standage - blog</title><link href="https://standage.github.io/" rel="alternate"></link><link href="https://standage.github.io/feeds/blog.atom.xml" rel="self"></link><id>https://standage.github.io/</id><updated>2019-12-31T00:00:00-05:00</updated><entry><title>Developer, pull request thyself</title><link href="https://standage.github.io/developer-pull-request-thyself.html" rel="alternate"></link><published>2019-12-31T00:00:00-05:00</published><updated>2019-12-31T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2019-12-31:/developer-pull-request-thyself.html</id><summary type="html">&lt;p&gt;I contend that you should be treating your solo software projects like a collaborative project and using pull requests, an issue tracker, and other social development tools.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; I contend that you should be treating your solo software projects like a collaborative project and using pull requests, an issue tracker, and other social development tools.&lt;/p&gt;
&lt;h2&gt;Preamble&lt;/h2&gt;
&lt;p&gt;I'm a huge fan of using &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; to maintain a versioned history of my software and research projects.
As an undergraduate I flirted briefly with &lt;a href="https://subversion.apache.org/"&gt;Subversion&lt;/a&gt;, but by the time I was taking version control seriously as a first-year grad student this new upstart company &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt; was driving rapid adoption of Git across the tech and science industries.
To anyone who has gotten into software more recently, it's difficult to effectively communicate the impact Git and GitHub have had on the practice of software development.
Leveraging Git's distributed design, GitHub provides a variety of collaborative development tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one-click "forking" to create your own personal copy of any public project,  over which you have full control&lt;/li&gt;
&lt;li&gt;"pull requests" for submitting proposed changes from your personal fork back to the original repository (for example, to fix a bug or add a new feature); alternatively, for repositories to which you have write access, you can make a pull request to merge a feature branch into the main "master" branch&lt;/li&gt;
&lt;li&gt;an integrated issue tracker that scans commit messages and to link issue threads to relevant commits and automagically close issues once they're resolved&lt;/li&gt;
&lt;li&gt;various other project management features like &lt;a href="https://help.github.com/en/github/managing-your-work-on-github/about-project-boards"&gt;project boards&lt;/a&gt; and &lt;a href="https://github.com/features/actions"&gt;continuous integration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The creators of Git and GitHub didn't invent any of these terms/concepts/strategies, but they deserve a lot of credit for creating an intuitive system that propelled their nearly universal adoption into the programmer's lexicon and daily practice.&lt;/p&gt;
&lt;p&gt;My own habits with respect to Git and GitHub have evolved in the last ~10 years.
This is due to a few factors: I was entirely self-taught at first; I gained experience over time contributing to a variety of open source software projects; and I began noticing and using some more recently introduced features of the GitHub platform itself.
In general, I don't claim any special expertise or profound insights into the &lt;em&gt;way of Git&lt;/em&gt;.
But there is one way I use GitHub that seems uncommon, and that is &lt;strong&gt;to make full use of its collaborative and social tools on projects to which I am the sole contributor&lt;/strong&gt;.
In other words, &lt;strong&gt;I collaborate with myself on solo projects&lt;/strong&gt;.
I contend that GitHub's collaborative development features are underused in solo software development projects, and that these projects have a lot to gain by adopting them.&lt;/p&gt;
&lt;h2&gt;Basically, an electronic lab notebook for software&lt;/h2&gt;
&lt;p&gt;My impression is that software developers don't generally make an active choice &lt;em&gt;not&lt;/em&gt; to use GitHub's collaborative tools for solo projects.
Rather, these tools are always taught and discussed in the context of communicating with others to coordinate updates to some central code base.
For projects with only a single contributor there's nobody else to coordinate or communicate with, so most folks will default to the simplest strategy and just commit &amp;amp; push directly to the master branch.
If someone is particularly careful or responsible, they might use branches to isolate new features or bug fixes from the master branch until they have been well tested.
But even then, merging a feature branch to master locally and then pushing directly to master on Github seems to be the common practice for solo projects.&lt;/p&gt;
&lt;p&gt;But let's consider for a moment what benefits GitHub's collaborative tools provide.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the ideal scenario, the need for each change (bug fix, refactoring, new feature, etc.) will first be described in a dedicated thread using the project's issue tracker.
This thread creates a dedicated space for project contributors to discuss the merits of different potential solutions.
If there is general consensus, somebody can make the proposed changes on a fork or branch and submit those changes as a pull request (PR).
The main text of the PR refers to the problem described in the issue thread (GitHub automatically detects this and adds hyperlinks to each thread), and describes how the proposed changes were made and/or how they will affect the software's behavior.
If there is &lt;em&gt;not&lt;/em&gt; consensus, alternative solutions can be implemented, described, and submitted for consideration simultaneously in separate PRs.
The discussion will continue as long as needed on the issue thread and any associated PR threads.
This sometimes requires back-and-forth communication with a PR contributor and the collaborators reviewing the PR, and multiple updates to a PR before it is approved (or rejected).
Once a PR is approved and merged into the master branch, the issue thread and PR threads are put into a "closed" or "resolved" state, but they remain a permanent searchable part of the GitHub repo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So what benefits do GitHub's collaborative tools provide?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;They document changes to the project.&lt;/strong&gt; In all areas, but especially in science, it's important to know how a software package's implementation and behavior have evolved over time. Keeping a change log is a common way this is done, but change logs are a very concise and limited summary of the changes. GitHub's issue threads and PRs capture the much richer context around each changeâ€”what motivated it, what solutions were considered, and how the solution was implemented. Even after the relevant threads are closed, they remain a browsable and searchable part of the repo's permanent record, available for future reference to anyone with interest and access.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;They force the developers to reflect on proposed changes in the context of the project's status and trajectory.&lt;/strong&gt; If a contributor can't clearly describe a bug or the need for a new feature, it's unlikely they will be able to convince their collaborators to approve a set of proposed changes to the software. Communicating clearly about issues and proposed solutions requires contributors to think holistically about the project, which goes a long way to preventing half-baked ill-advised changes to the code that end up creating more problems down the road than they solve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;They foster transparency in the development process.&lt;/strong&gt; When all updates to the software are submitted as PRs, and when each PR refers to a specific problem described in sufficient detail, and when each PR must be reviewed before it is approved and merged, there are multiple layers of defense against a single contributor making unilateral changes without consulting collaborators.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don't think I have to work hard to convince anybody that these benefits are just as important for solo projects as they are for collaborative projects.
It's common for scientists to work on multiple projects at a time, and for each project to span months or years.
A scientist displays a lot of hubris when they claim that they can responsibly stay on top of this without &lt;strong&gt;documenting changes&lt;/strong&gt;, &lt;strong&gt;reflecting on proposed changes in the context of the project's status and trajectory&lt;/strong&gt;, and &lt;strong&gt;fostering transparency in their development process&lt;/strong&gt;.
Many colleagues and acquaintances can relate when I share my frustration that I routinely struggle to understand code I wrote 2 months ago.
So if you ever plan on using your software on a continuous basis, or using it again some time in the near future, or reusing portions of it later for a new project, or, you know, re-analyzing your data when the reviewers raise concerns about the Methods section of your research manuscript, it's important to have something to fall back on.&lt;/p&gt;
&lt;p&gt;For scientists working in a lab, keeping a detailed lab notebook is a professional (and, depending on the field, sometimes a legal) obligation.
But for some reason, this doesn't often get translated into the realm of scientific software.
I suggest that using GitHub's collaborative features as described above is a great way to maintain an electronic lab notebook for a software project, even if you are the only contributor.&lt;/p&gt;
&lt;h2&gt;How about some examples?&lt;/h2&gt;
&lt;p&gt;Here are a few examples from solo projects I've worked on recently.
In each case, a dedicated issue thread was created to describe a bug or new feature, and later a PR to resolve that issue was submitted, reviewed (by myself!), and merged.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example 1: &lt;a href="https://github.com/kevlar-dev/kevlar/issues/360"&gt;issue&lt;/a&gt; and &lt;a href="https://github.com/kevlar-dev/kevlar/pull/361"&gt;PR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 2: &lt;a href="https://github.com/bioforensics/MicroHapDB/issues/43"&gt;issue&lt;/a&gt; and &lt;a href="https://github.com/bioforensics/MicroHapDB/pull/46"&gt;PR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 3: &lt;a href="https://github.com/bioforensics/MicroHapulator/issues/52"&gt;issue&lt;/a&gt; and &lt;a href="https://github.com/bioforensics/MicroHapulator/pull/53"&gt;PR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Occasionally I'll notice a bug, begin investigating it, and start implementing a fix before documenting the bug in a dedicated issue thread.
In these cases, I'll just provide a description of the bug in the main PR text.
Here's &lt;a href="https://github.com/bioforensics/MicroHapulator/pull/70"&gt;an example&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Make it collaborative!&lt;/h2&gt;
&lt;p&gt;For solo projects where you're the only one doing the coding, it's still likely that your project is influenced directly or indirectly by interactions with your colleagues, advisors, and/or trainees.
Very little scientific work is done in total isolation.&lt;/p&gt;
&lt;p&gt;So if you're using GitHub's issue tracker to describe bugs to be fixed and features to be implemented, and if you're using pull requests (PRs) to describe and evaluate updates to the code, it's very easy to turn your solo project into a collaborative project!
Here are a few ideas about how you might do that.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You've created a new branch in your git repo to implement and test a new feature for your software. You've submitted a PR to merge this branch into the master branch. &lt;strong&gt;Send a link to this PR to your collaborator and ask if she has any feedback on your proposed changes.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You recently noticed a bug in your software's behavior. You briefly described the bug in a new thread in the issue tracker, but you haven't had time to go back and fix it yet. &lt;strong&gt;Send a link to the bug report to your rotation student, give him access to your repo, and invite him to create a new branch to fix the bug and then submit changes as a pull request.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In your last meeting with your advisor, she expressed concerns with some details of your software's core algorithm. &lt;strong&gt;Take time to consider her concerns, write them down in a new issue thread, and then send her a link to the thread and ask if you've understood her concerns correctly.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Is it worth it?&lt;/h2&gt;
&lt;p&gt;Scientific software comes in many shapes and sizes.
You might implement software as a proof-of-concept, tailored for a specific task unique to a study you're conducting.
Alternatively, you might write a program that implements a method that you think would be useful to numerous scientists in a variety of similarly designed studies.
Or perhaps you implement your software as a library, intended to help other scientists write their own scripts and programs for simulating and analyzing data.&lt;/p&gt;
&lt;p&gt;If your aspiration is for your solo software project to become widely used, then treating it like it's already a collaborative project is a no-brainer.
(Whether it is &lt;em&gt;actually&lt;/em&gt; widely used depends on a variety of factors, some beyond your control: how easy it is to install; how helpful the documentation is; the journal(s) in which you've published papers describing it, and the results/findings described in those papers; who notices, uses, and cites it, or discusses it on social media.)&lt;/p&gt;
&lt;p&gt;But even if you &lt;em&gt;don't&lt;/em&gt; aspire for a particular solo software project to become widely used, I still propose that it is worth your time to treat it as a collaborative project.
There are a few good reasons for this.
While it may require a bit of time and deliberate effort at first, this time and effort will decrease as it becomes a habit.
And becoming comfortable with Git and GitHub's collaborative features will make it much easier for current or future projects that you &lt;em&gt;do&lt;/em&gt; hope will be widely used and recognized.
These same skills will also come in handy if you ever want to contribute to an open source software project, science related or not.&lt;/p&gt;
&lt;p&gt;In short: it doesn't take much extra time, especially once you've made a habit of using skills that will serve you well in a variety of contexts in the future.&lt;/p&gt;
&lt;h2&gt;But won't I get scooped?&lt;/h2&gt;
&lt;p&gt;Statistically, the likelihood that your software will go largely unrecognized is much higher than the likelihood that someone will follow your GitHub repo closely, steal your ideas, and publish a paper describing your work before you can.
I certainly think that, as a general rule, scientists are best served by developing software in the open from the start.&lt;/p&gt;
&lt;p&gt;But if you think you have valid concerns about making your software public at first, GitHub allows you to keep your repository private.
You can use all of the strategies discussed here, and for private repos visibility will be limited to you and those you specifically choose (if any).
Then, when you are ready to share your software more broadly, you can mark the repo as public.&lt;/p&gt;
&lt;p&gt;If you publish a paper describing or referencing the software, you should definitely make the repo public by manuscript submission time at the latest.
Links to a private repository do not allow anonymous peer review, and "software available upon request" statements are completely inappropriate.&lt;/p&gt;
&lt;h2&gt;Recommendations&lt;/h2&gt;
&lt;p&gt;To summarize, let me just clarify and reiterate a few recommendations for solo software development projects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use GitHub's issue tracker liberally.&lt;/strong&gt; Describe bugs to be fixed, features to be implemented, questions to ponder, and so on. You don't have to be verbose, just make sure that what you've written is clear.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Don't push changes directly to the master branch.&lt;/strong&gt; You can even have GitHub prevent you from doing this with your repository's &lt;a href="https://help.github.com/en/github/administering-a-repository/configuring-protected-branches"&gt;Branch Protection Rules&lt;/a&gt;. Instead, whenever you need to make an update, create a new branch, commit changes to that branch, push the branch to your GitHub repo, and submit the changes as a pull request (PR).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Review your own PRs before you merge them.&lt;/strong&gt; The PR thread has a "diff" view that displays the differences between your new branch and the master branch. Examine these differences carefully. Did any changes sneak through that shouldn't have been included? Did you forget to remove temporary &lt;code&gt;print&lt;/code&gt;, &lt;code&gt;assert&lt;/code&gt;, or other debugging statements? GitHub PRs also allow you to &lt;a href="https://stackoverflow.com/a/58190804/459780"&gt;select several lines of code in the diff and comment on those lines&lt;/a&gt;. Use these to:&lt;ul&gt;
&lt;li&gt;highlight the most important changes in a PR; i.e. "&lt;em&gt;this change fixes the integer underflow bug&lt;/em&gt;"&lt;/li&gt;
&lt;li&gt;mark problems to be resolved; i.e. "&lt;em&gt;whoops, this needs to be fixed before the PR is merged&lt;/em&gt;")&lt;/li&gt;
&lt;li&gt;call out particularly messy or complicated code blocks; i.e., "&lt;em&gt;this works for now, but I should probably go back and clean this up when I have time&lt;/em&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compile an automated test suite.&lt;/strong&gt; I've &lt;a href="https://standage.github.io/the-joy-and-art-of-automated-testing.html"&gt;written previously about my strong opinions on testing scientific software&lt;/a&gt;. All the better if you can configure GitHub to run the automated test suite for you each time you create or update a PR. You can even configure GitHub to prevent PRs from being merged until the test suite passesâ€”I highly recommend!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Try to find ways to involve others in your project.&lt;/strong&gt; Trainees, advisors, colleagues, and even acquaintances are often willing to respond favorably to a considerate and clearly communicated request to provide feedback, perform a brief review, or a collaborate on a software development task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Postscript&lt;/h2&gt;
&lt;p&gt;I've focused my discussion here entirely on GitHub, since it is by far the most popular platform for open source software and scientific software.
Alternatives exist however, such as GitLab and BitBucket.
All of the strategies discussed here should work regardless of what platform you're using.
In some cases, the terminology may be slightly different.
For example, GitLab uses the term &lt;strong&gt;merge request&lt;/strong&gt; instead of &lt;strong&gt;pull request&lt;/strong&gt;.&lt;/p&gt;</content><category term="software"></category><category term="social"></category></entry><entry><title>A snake in the pipes!</title><link href="https://standage.github.io/a-snake-in-the-pipes.html" rel="alternate"></link><published>2018-12-26T00:00:00-05:00</published><updated>2018-12-26T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2018-12-26:/a-snake-in-the-pipes.html</id><summary type="html">&lt;p&gt;A few comments on the new "pipe" output flag in Snakemake.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently while consulting the &lt;a href="https://snakemake.readthedocs.io"&gt;Snakemake documentation&lt;/a&gt; (a common occurrence for me these days) I noticed a &lt;a href="https://snakemake.readthedocs.io/en/v5.4.0/snakefiles/rules.html#piped-output"&gt;new feature&lt;/a&gt; I had never encountered before.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;From Snakemake 5.0 on, it is possible to mark output files as pipes, via the &lt;code&gt;pipe&lt;/code&gt; flag...If an output file is marked to be a pipe, then Snakemake will first create a &lt;a href="https://en.wikipedia.org/wiki/Named_pipe"&gt;named pipe&lt;/a&gt; with the given name and then execute the creating job simultaneously with the consuming job...&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Named pipes are much less commonly used than the pipe (or &lt;code&gt;|&lt;/code&gt;) character in UNIX land, but they serve the same purpose: rather than writing the output of one command to the terminal or to file on disk, send it to another command. The difference is that named pipes have a user-specified filename on the file system. But if you have &lt;code&gt;command1&lt;/code&gt; printing to a named pipe and &lt;code&gt;command2&lt;/code&gt; reading from it, it will act just as if you had executed &lt;code&gt;command1 | command2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since discovering this new feature, I've tried it in a few workflows. It has proven useful in a couple of scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Streaming non-&lt;code&gt;stdout&lt;/code&gt; output&lt;/strong&gt;: Some programs do not provide the option to write output to &lt;code&gt;stdout&lt;/code&gt;, which makes it more difficult to execute in a streaming fashion. However, if you mark a rule's output files with &lt;code&gt;pipe&lt;/code&gt; and then provide these as input to another rule, the data will be passed from rule to rule via the named pipes and will never touch disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Breaking up complex pipelines&lt;/strong&gt;: Snakemake will happily execute shell commands with several subcommands piped together. But when each command has a handful of parameters, configuring this pipeline in a single rule can become cumbersome. I've found in some cases that things are much cleaner and more decipherable when the pipeline is split across multiple rules using &lt;code&gt;pipe&lt;/code&gt;'d files as data intermediates. I've successfully connected 3 commands using pipe intermediates, so presumable there should be no issues in piping together any arbitrary number of rules with this mechanism.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What do you use the &lt;code&gt;pipe&lt;/code&gt; flag for?&lt;/p&gt;</content><category term="snakemake"></category><category term="pipes"></category><category term="python"></category></entry><entry><title>The Joy and Art of Automated Testing</title><link href="https://standage.github.io/the-joy-and-art-of-automated-testing.html" rel="alternate"></link><published>2018-11-19T00:00:00-05:00</published><updated>2018-11-19T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2018-11-19:/the-joy-and-art-of-automated-testing.html</id><summary type="html">&lt;p&gt;A primer on software testing for scientists and researchers.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;center&gt;
&lt;img alt="Retro tea cup advert: Serve up your software hot, fresh, and reliable with AUTOMATED TESTING!" src="https://i.imgflip.com/2lbq54.jpg"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;If you're in the business of writing software, one of your primary concerns is whether your software does its intended job correctly. If, like me, you're in the business of writing software in a research or science context, ensuring the reliability of the software is of &lt;em&gt;critical&lt;/em&gt; importance. Peer review of changes or updates to the code, in chunks of manageable size, is one of the best ways to find bugs and improve software quality. But sooner or later, you're going to need to &lt;strong&gt;test&lt;/strong&gt; your software.&lt;/p&gt;
&lt;p&gt;The purpose of this post is not to convince you that testing is an important professional obligation. If writing software is your daily bread, then this should already be a given. If, instead, you're a scientist or researcher that finds themself spending an increasing amount of time dabbling in code, think of testing as the positive and negative controls you design to ensure your experimental results are reliable. It's just not something you can compromise on without threatening the integrity of the entire process. (Note: you're likely engaged in some amount of testing already! If you execute code and evaluate its behavior or compare a result against your expectations, that's testing. Good for you!)&lt;/p&gt;
&lt;p&gt;Rather, the purpose of this post is to persuade you that you can and probably should be testing more frequently and more comprehensively, and that automation is your friend. I discuss some of the concepts and strategies that make testing effective and that make it possible to test frequently without becoming overwhelmed. Perhaps, &lt;em&gt;you too&lt;/em&gt; may come to more fully appreciate the joy and art of automated testing! :-)&lt;/p&gt;
&lt;h2&gt;What is "correct"?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Physicists worry about decimal places. Astronomers worry about exponents. Economists are happy if they've got the sign right."&lt;/em&gt;&lt;br&gt;
--Someone wise&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The purpose of testing is to verify that your software does its job correctly. Of course, what &lt;em&gt;correct&lt;/em&gt; means will depend on the nature of the problems you work on and the types of tests you're writing. If your research focus is incrementally improving solutions to well-understood problems, then there are likely "gold standard" reference data sets that have been studied extensively by your community and where the "answers" are well known. If you're working on poorly understood cutting-edge problems in a novel discipline, you may have to rely more on deriving correct answers from first principles. Most researchers fall somewhere between these two extremes, and you should use your judgement and experience (and suggestions from trusted colleagues, if unsure) to determine what inputs to use for your tests and what results should be declared correct. For some problems an exact answer is required, while in other cases an exact solution is not feasible and an answer within a small numerical range is acceptable. This range defines the tolerance for error (or simply &lt;em&gt;tolerance&lt;/em&gt;) of your test.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete numerical results often require an exact integer answer. Sometimes, a small range of integers can be specified.&lt;/li&gt;
&lt;li&gt;Text results also typically require an exact answer. Sometimes, a regular expression can be specified to indicate a particular pattern that must be satisfied.&lt;/li&gt;
&lt;li&gt;Continuous numerical values cannot be stored exactly even on modern computers, so evaluating floating point numerical results requires specifying an accepted tolerance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are numerous alternative ways to assess a result that may be more appropriate in some circumstances: set membership, similarity to a reference, number of results produced, etc. Ideally, you want to make the test reasonably stringent without causing failures for trivial reasons. And as you continue to test your software over time, you'll get a better feel for this balance and gradually tighten the stringency of your tests.&lt;/p&gt;
&lt;h2&gt;Test automation&lt;/h2&gt;
&lt;p&gt;Once you have successfully completed the test(s), the next step is to &lt;em&gt;automate&lt;/em&gt; test execution. It may be tempting to convince yourself that it is not worth the time required to automate your testing, but as you develop your software you will need to execute the tests over and over again to make sure your bug fixes and optimizations and new features don't introduce new bugs in the code.&lt;/p&gt;
&lt;p&gt;Ideally, you should be able to execute your tests (or a relevant subset of tests) in as few keystrokes as possible. For each individual test, you write code to call the function/subroutine/module/script/program you want to test, store the result, and compare the result to the "correct" answer as discussed in the previous section. Then, you should have one master script or command that will run all of these tests for you. This is what we mean by &lt;strong&gt;test automation&lt;/strong&gt; and &lt;strong&gt;automated testing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Many programming languages have testing frameworks that provide convenient features and eliminate a lot of the pain associated with writing and running tests. I'll discuss these a bit in the &lt;strong&gt;Testing Frameworks&lt;/strong&gt; section below.&lt;/p&gt;
&lt;h2&gt;Types of tests&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Computers are useless. They can only give you answers."&lt;/em&gt;&lt;br&gt;
--Pablo Picasso&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a few different testing strategies you can use to evaluate different aspects of your software. These are not competitive or exclusive: sometimes a test doesn't fit cleanly into just one of these categories, and you'll want to use a combination of these approaches in your test suite.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;smoke tests&lt;/strong&gt;: Here, you're simply asking &lt;em&gt;"Can I run this bit of code without things blowing up?"&lt;/em&gt; Rather than testing that the software produces a specific result, it ensures the software can be run under normal conditions without producing a runtime error. This is a crude testing technique, but a valuable and easy one particularly appropriate for use early in the development process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;unit tests&lt;/strong&gt;: A unit test, well, tests a particular unit of code. (I know, deep stuff!) There are no strict rules about what qualifies as a "unit" of code. Often, unit tests are focused on functions, subroutines, or methods, and are intended to give you and other developers confidence that the function works as advertised. This will enable you to treat the function as a magic black box when calling it elsewhere in your code, allowing you to give all of your focus to the code you're currently working on. If you test each "unit" of your code individually, you'll find later on that fitting those units together into a larger program/workflow/pipeline comes with much less struggle.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;functional tests&lt;/strong&gt;: Functional tests focus on larger-scale operations and behaviors implemented by the code. Again, there are no universally-defined rules about what does and does not qualify as a functional test, but tests for entire modules, scripts or programs often fall under this category. Functional tests focus on end users: what kind of operations will the end user perform with the software, and do these operations produce correct results? These kinds of tests are especially valuable when sharing your code with others, as they can determine whether the software has been set up and installed correctly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;regression tests&lt;/strong&gt;: Regression tests make sure that continued development of your code doesn't break features that are already working correctly. Regression tests are great for making sure that when you fix a bug, it &lt;em&gt;stays&lt;/em&gt; fixed. More generally, regression testing can give you the confidence to reorganize large portions of your code (for example, to support new features) without worrying whether you're breaking the features that already work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;doctests&lt;/strong&gt;: A doctest is a test that is embedded within a comment or documentation string in the code. Doctests serve dual purposes: they serve as user documentation, showing how to invoke certain operations; doctests also show how the code should behave on certain inputs in a way that can be automatically tested. This strategy is not supported in all languages, but can be valuable when available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Testing frameworks&lt;/h2&gt;
&lt;p&gt;All of the most popular programming languages have testing frameworks designed to make it easier for you to write and run tests. Personally, I'm partial to the &lt;a href="https://docs.pytest.org"&gt;pytest&lt;/a&gt; framework because&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I write the majority of my code in Python; and&lt;/li&gt;
&lt;li&gt;the project is actively developed and responsive to bug reports and help requests; and&lt;/li&gt;
&lt;li&gt;it has all the features I'll ever need for testing (and more)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, there are similarly convenient and modern testing frameworks available for R, Julia, C++, Java, and many other languages. If you're new to testing, the factors that will probably make the most difference to you are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;how easy is the framework to install? and&lt;/li&gt;
&lt;li&gt;how active is the community using this framework? how responsive will they be to my questions?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Typically, a test framework works as follows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You implement each test as an independent function, usually invoking &lt;code&gt;assert&lt;/code&gt; statements.&lt;/li&gt;
&lt;li&gt;Each test produces a PASS, FAIL, or ERROR result. If all &lt;code&gt;assert&lt;/code&gt; statements pass, the test PASSes. If any &lt;code&gt;assert&lt;/code&gt; statement fails, the test FAILs. If there is a problem executing the test, the test ERRORs.&lt;/li&gt;
&lt;li&gt;Optionally, you can specify "fixtures" or "setup/teardown" procedures to make test data available to each test. Data that is reused by multiple tests within a module can be loaded with a single module-level fixture, while data that is used by multiple modules might by loaded with a single global fixture. Appropriately-scoped fixtures or setup/teardown procedures make sure that time isn't wasted loading test data multiple times.&lt;/li&gt;
&lt;li&gt;There is a "test runner" that finds the tests (from a config file or by traversing the file structure), executes each test, and reports the aggregate results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If these concepts seem new and overwhelming, don't be intimidated. Start with small tests to understand the mechanics of your chosen framework, and then escalate gradually to more complex tests as you gain experience and confidence.&lt;/p&gt;
&lt;h2&gt;Test coverage&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;QA Engineer walks into a bar. Orders a beer. Orders 0 beers. Orders 999999999 beers. Orders a lizard. Orders -1 beers. Orders a sfdeljknesv.&lt;/em&gt;&lt;br&gt;
-- Bill Sempf (&lt;a href="https://twitter.com/sempf/status/514473420277694465"&gt;@sempf&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many testing frameworks can report &lt;em&gt;test coverage&lt;/em&gt;, or the precise lines of code that are (and are not) invoked during test execution. This valuable information will help you identify potentially risky regions of your codebase. If a line of code is never run during test execution, you can't really be confident it works correctly. In fact, you can't even be sure it won't cause a runtime error and halt program execution right away. The higher your test coverage, the more confidence you can have that your software is going to 1) run; and 2) do its job correctly. If there are large blocks of important code that have no test coverage, you should consider writing tests that will invoke these lines of code.&lt;/p&gt;
&lt;p&gt;If you can get 100% test coverage for your software, that's tremendous. Often this isn't feasible, and regardless it doesn't mean your software has no defects. Aiming for 80-90% coverage is usually a healthy target in my experience. Regardless, the overall percent coverage is only part of the story (more on that below). The take-home message is that you want to avoid large drops in coverage as you update your code over time. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: It's also worth mentioning that the notion of test coverage discussed above, while common and useful, is also a bit shallow. Let's call it &lt;em&gt;statement coverage&lt;/em&gt;. In the strictest sense, statement coverage can really only tell you which lines of code were executed without causing an unexpected error. They &lt;strong&gt;do not&lt;/strong&gt; measure how much thought you put into your tests, what your tests actually measure, or how comprehensively your tests handle the full range of possible inputs. To illustrate, imagine writing lots of smoke tests that collectively invoke every function and instantiate every object in your codebase without examining any outputs or behavior. This approach could achieve very high statement coverage, maybe even 100%! But coverage would be a pretty useless measure of the software's reliability in this case since the tests don't actually, you know, test anything.&lt;/p&gt;
&lt;p&gt;In traditional software engineering, a lot of time is spent up front drafting formal requirements in precise technical language to describe what types of inputs a software product will accept, the range of values those inputs can take, the outputs the software will produce, and the acceptable level of precision. Most scientists are not trained in this kind of requirements planning. In any case, this level of formality is poorly suited to research science, where our conceptual models of the problem space are typically rudimentary, incomplete, and rapidly evolving. The point, again, is that 100% statement coverage is not a guarantee that your code will work correctly for all valid inputs. The more thought and effort you put in to designing inputs, outputs, tolerances, and edge cases for your tests, the more closely your test coverage will reflect the true and practical (but difficult-to-measure) accuracy and reliability of your software.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ANOTHER NOTE&lt;/strong&gt;: If you're feeling particularly adventurous, property-based testing strategies provide an effective means to address software quality in a fairly rigorous way without resorting to formal engineering methods. The idea is that you write a test not knowing what the input data will be, instead testing that the output has certain properties that should be true &lt;em&gt;in every case&lt;/em&gt; (i.e. a numerical operation should be commutative, or the number of distinct outputs depends on an easily computed characteristic of the input). You then invoke the test with a large number of (possibly randomly generated) inputs and see whether the code you're testing works correctly on each inputâ€”see the joke above about the QA engineer. To the extent you use property-based testing in your test suite, your test coverage will be a much better reflection of the true accuracy of your software. To learn more, do a Google search for "property based testing python", replacing "python" with your favorite language, of course!&lt;/p&gt;
&lt;h2&gt;Test-driven development&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Technology is a word that describes something that doesn't work yet."&lt;/em&gt;&lt;br&gt;
--Douglas Adams&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea behind &lt;em&gt;test-driven development (TDD)&lt;/em&gt; is to write tests that don't pass or even run initially because the code they are intended to test doesn't exist yet. This might seem like a backwards approach to writing software, but TDD advocates claim it is a more responsible strategy. They claim it forces you to formulate the correctness tests &lt;em&gt;a priori&lt;/em&gt;, and thus help avoid confirmation bias. They also claim that it forces you spend more time upfront considering how code will be structured and how data will be passed around, since tests cannot be written without these considerations.&lt;/p&gt;
&lt;p&gt;While these sound like plausible benefits, empirical studies testing the effectiveness of TDD are as yet inconclusive. Many proficient programmers swear by TDD, so it's definitely worth giving an honest and deliberate try, especially if you struggle to find time to go back and write tests for your code. But if you're disciplined and have success using a different strategy, don't stress about TDD too much. :-)&lt;/p&gt;
&lt;p&gt;Regression tests for bugfixes provide a particularly appropriate opportunity to put TDD into practice. When a bug is found in your code, your first task is to reproduce the bug. Once you've successfully done so, TDD prescribes that you write a new test to isolate and reproduce the faulty behavior. At first, this new regresssion test will fail. That's the idea, since you haven't fixed the bug yet! Once the regression test is complete and accurate, &lt;em&gt;then&lt;/em&gt; you can go update your code to fix the bug. You'll know it's fixed when your new regression test finally passes!&lt;/p&gt;
&lt;h2&gt;Continuous integration and peer review&lt;/h2&gt;
&lt;p&gt;Whether you update your code on a frequent basis or only touch it occasionally, &lt;em&gt;continuous integration (CI)&lt;/em&gt; is one of the best ways to ensure your software is always in a runnable state. There are two complementary approaches to CI: automatically running tests as a "cron job" on a schedule (nightly, weekly, etc.) and automatically running tests whenever an update to the code is committed. If you've posted your code on Github or Gitlab, there are a variety of free CI services (such as &lt;a href="https://travis-ci.org"&gt;Travis CI&lt;/a&gt;, &lt;a href="https://drone.io/"&gt;Drone.io&lt;/a&gt;, and &lt;a href="https://circleci.com/"&gt;Circle CI&lt;/a&gt;) that will run your tests for you, post the results, and send notifications if the test build fails. &lt;a href="https://jenkins.io/"&gt;Jenkins&lt;/a&gt; is a similar service that you can download and install for free on your own system if you're restricted from posting your code publicly.&lt;/p&gt;
&lt;p&gt;If you're lucky enough to have colleagues or collaborators contributing to your software, you've probably established some kind of protocol for peer review of code, even if it's informal and &lt;em&gt;ad hoc&lt;/em&gt;. Code review is big and important topic and out of scope for this post, so I'll just briefly mention that CI can be an extremely valuable asset in the code review process. The Github "pull request" mechanism allows you to review any proposed changes to your code before you finalize them, and will also trigger a CI build. In addition to your typical review process, you should require at a minimum that all tests pass before you accept any proposed changes.&lt;/p&gt;
&lt;h2&gt;Compromise&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Given the pace of technology, I propose we leave math to the machines and go play outside."&lt;/em&gt;&lt;br&gt;
--Bill Watterson (Calvin from &lt;em&gt;Calvin and Hobbes&lt;/em&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a concluding thought, I'll just re-emphasize the sentiment that we write software &lt;em&gt;to solve problems&lt;/em&gt; and &lt;em&gt;to more clearly understand the world&lt;/em&gt;. Within any project, some of the code you write is directly related to the software's purpose, while much of the rest is involved in sundry routine and uninteresting operations: opening files, reading data into memory, parsing arguments or config files, and so on. I'm not going to say that these other bits of code are unimportant, but if one is not careful it's possible to spend an inordinate amount of time applying and re-applying all kinds of software engineering best practices to portions of the code that, at the end of the day, contribute very little to science. When time and resources and experience are limited, it's important that we focus our energy on the most important priorities.&lt;/p&gt;
&lt;p&gt;A few years ago, &lt;a href="https://twitter.com/ctitusbrown"&gt;Titus Brown&lt;/a&gt; wrote a great post on what he calls &lt;a href="http://ivory.idyll.org/blog/2014-research-coding.html"&gt;"stupidity driven development."&lt;/a&gt; He argues that inaccurate results have much more dire consequences than software crashes, and thus he and his colleagues delayed fixing some annoying bugs for quite a while so that they could instead focus on the science questions they were using the code to explore. I think this is a winning strategy. The scientific core of your code deserves all the time, attention, and "best practices" you can afford to invest in it. The rest doesn't deserve any more than you feel like giving it, and I hereby grant you permission to compromise on any of the strategies I've discussed to the extent that doing so helps you focus more on testing the scientifically critical portions of your code more rigorously!&lt;/p&gt;
&lt;p&gt;On the other hand, some of the most widely used (and widely cited!) software tools in my field (genomics and bioinformatics) initially gained their notoriety from the fact that in an ocean of poorly documented and unusable tools they could actually compile and run on most people's systems without too much hassle. Time spent improving the "user experience" for your software may not make for better science directly, but it &lt;em&gt;may&lt;/em&gt; make it easier for others to appreciate and build on your science.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This post is based on material developed by &lt;a href="https://twitter.com/gvwilson"&gt;Greg Wilson&lt;/a&gt; and myself for teaching &lt;a href="https://software-carpentry.org/"&gt;Software Carpentry&lt;/a&gt; workshops on &lt;a href="https://swcarpentry.github.io/managing-research-software-projects/"&gt;managing research software projects&lt;/a&gt;. Greg is a great mentor and friend and I want thank him for his consistent generosity and for simply being A Good Guy. I'd also like to thank Morgan Taschuk (&lt;a href="http://twitter.com/morgantaschuk/"&gt;@morgantaschuk&lt;/a&gt;), Luiz Irber (&lt;a href="https://twitter.com/luizirber"&gt;@luizirber&lt;/a&gt;), and Titus Brown (&lt;a href="https://twitter.com/ctitusbrown"&gt;@ctitusbrown&lt;/a&gt;) for insightful comments on early drafts of this blog post.&lt;/p&gt;</content><category term="software"></category><category term="testing"></category></entry><entry><title>A brief review of HULK and histosketch</title><link href="https://standage.github.io/a-brief-review-of-hulk-and-histosketch.html" rel="alternate"></link><published>2018-10-12T00:00:00-04:00</published><updated>2018-10-12T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2018-10-12:/a-brief-review-of-hulk-and-histosketch.html</id><summary type="html">&lt;p&gt;About a month ago, I was intrigued to see a bit of Twitter activity around a new bioRxiv preprint.
The &lt;a href="https://www.biorxiv.org/content/early/2018/09/04/408070"&gt;manuscript&lt;/a&gt; describes &lt;a href="https://github.com/will-rowe/hulk"&gt;HULK&lt;/a&gt;, a new bioinformatics tool that implements some useful comparison metrics and operations for analyzing (meta)genomes.
HULK is based on a new algorithm called histogram sketching (HistoSketch for short), following the trend of related sketching algorithms (HyperLogLog â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;About a month ago, I was intrigued to see a bit of Twitter activity around a new bioRxiv preprint.
The &lt;a href="https://www.biorxiv.org/content/early/2018/09/04/408070"&gt;manuscript&lt;/a&gt; describes &lt;a href="https://github.com/will-rowe/hulk"&gt;HULK&lt;/a&gt;, a new bioinformatics tool that implements some useful comparison metrics and operations for analyzing (meta)genomes.
HULK is based on a new algorithm called histogram sketching (HistoSketch for short), following the trend of related sketching algorithms (HyperLogLog, CountMin, MinHash, etc.) that have steadily captured the collective interest of the genome informatics community over the last several years.&lt;/p&gt;
&lt;p&gt;I recently led a journal club to discuss the histogram sketching algorithm and to discuss the claims and results discussed in the HULK paper.
I'm posting some notes from the journal club here mostly for my own sake, but I'd be elated if they happen to be insightful to anyone else who is interested in learning more about the tool or the topic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: While a couple of the claims in the manuscript should be qualified, HULK's treatment of &lt;em&gt;k&lt;/em&gt;-mer abundances and mismatched data set sizes is indeed novel, and incremental sketching + machine learning = awesome!&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The terms &lt;em&gt;sketch&lt;/em&gt; and &lt;em&gt;sketching algorithm&lt;/em&gt; seem to be used fairly frequently (at least over the last several years), but surprisingly I was unable to find an authoritative definition for them.
My understanding is that a sketch is a compact approximation of some characteristic of a large data set, just as a pencil sketch approximates a photograph or a painting.
Different kinds of sketches approximate different characteristics of the data set: the HyperLogLog approximates the cardinality (size, or number of distinct elements) of the dataset; the Bloom filter can provide approximate membership queries ("&lt;em&gt;Is this element in the data set?&lt;/em&gt;"); the CountMin sketch provides approximate frequency queries ("&lt;em&gt;How many times does this element occur in the data set?&lt;/em&gt;"); and the MinHash sketch selects a representative sample of set elements to...well, represent the data set! The number of shared elements between two MinHash sketches, it turns out, is an excellent proxy for the "distance" or similarity between the data sets.&lt;/p&gt;
&lt;p&gt;While all of these sketches have useful bioinformatics applications, I think it's safe to say that MinHash in particular is making big splashes in the genomics community.
MinHash sketches require a negligible amount of storage, can be computed extremely quickly, and permit similarity calculations that can enable or improve metagenome analysis, genome assembly, sample ID, and more.
In addition to the &lt;a href="https://doi.org/10.1186/s13059-016-0997-x"&gt;trend-setting Mash paper&lt;/a&gt; and &lt;a href="https://github.com/marbl/mash"&gt;corresponding tool&lt;/a&gt;, MinHash sketching has been re-implemented in &lt;a href="https://github.com/dib-lab/sourmash"&gt;sourmash&lt;/a&gt; and &lt;a href="https://github.com/onecodex/finch-rs"&gt;Finch&lt;/a&gt; (and probably others) to facilitate research into additional (meta)genomics applications of this class of algorithms.&lt;/p&gt;
&lt;h2&gt;Histogram sketching&lt;/h2&gt;
&lt;p&gt;The histogram sketching algorithm was first published in the &lt;a href="https://doi.org/10.1109/ICDM.2017.64"&gt;proceedings of the 2017 IEEE data mining conference&lt;/a&gt;.
Like most sketching algorithms, it was not originally developed with biology applications in mind.
The authors sought a way to efficiently compare and classify &lt;em&gt;continuous streams&lt;/em&gt; of histogram data, such as from customer visits to a restaurant or other business.
In particular, the authors wanted to account for &lt;em&gt;concept drift&lt;/em&gt;, in which "the underlying distribution of a streaming histogram changes over time in unforseen ways."&lt;/p&gt;
&lt;p&gt;The authors of this work developed HistoSketch to address these needs.
A HistoSketch is similar to MinHash in that it stores a small, fixed number of histogram elements.
A weight is assigned to each histogram element in the sketch using an approach termed &lt;em&gt;Consistent Weighted Sampling (CWS)&lt;/em&gt;.
The math that defines CWS is a bit above my pay grade, but it endows HistoSketch with some of the properties that sets it apart from alternative sketching algorithms.
In particular, HistoSketch is a &lt;em&gt;similarity-preserving&lt;/em&gt; data structure while CountMin, for example, is not&lt;sup&gt;â€¡&lt;/sup&gt;.
And while MinHash &lt;em&gt;is&lt;/em&gt; similarity-preserving, it only accounts for the elements of the data set and not the histogram of the elements' frequencies.
HistoSketch defines a similarity-preserving data structure that is frequency-sensitive, capturing the "best of both worlds" while also elegantly addressing the issue of concept drift that other algorithms do not account for.&lt;/p&gt;
&lt;h2&gt;HULK&lt;/h2&gt;
&lt;p&gt;Which brings us to the main event.
HULK is a new bioinformatics tool that implements histogram sketching in the context of DNA sequence data.
HULK computes a histosketch of the &lt;em&gt;k&lt;/em&gt;-mer spectrum from a stream of data in Fastq or Fasta format.
The histosketches computed by HULK can be used for similarity calculations based on the weighted variant of the Jaccard distance metric employed by Mash and company.
Additionally, the authors demonstrate that histosketches can be integrated with machine learning models to classify microbiome samples based on the body site from which they were collected.
The experiments described in the manuscript demonstrate that even incomplete/intermediate sketches of a data stream are sufficient to cluster and classify microbiome samples.&lt;/p&gt;
&lt;p&gt;HistoSketch and HULK are clearly a promising development in this area, but the million dollar question is what precise advantages they provide over alternative algorithms.
The authors of the HULK preprint claim that histosketch comes with the following advantages.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;speed&lt;/li&gt;
&lt;li&gt;streaming support (incremental sketching)&lt;/li&gt;
&lt;li&gt;robustness for comparing samples of significantly different sizes&lt;/li&gt;
&lt;li&gt;sensitivity to &lt;em&gt;k&lt;/em&gt;-mer frequencies&lt;/li&gt;
&lt;li&gt;machine learning integration&lt;/li&gt;
&lt;li&gt;elegant handling of "concept drift"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on a careful study of the manuscript, a bit of email correspondence with the primary author, a partial code review, and discussion in our journal club, I'd like to provide my assessment of each of these claims.&lt;/p&gt;
&lt;h3&gt;Speed&lt;/h3&gt;
&lt;p&gt;The superior speed of HULK is not something that was emphasized &lt;em&gt;too much&lt;/em&gt; in the preprint.
It was mentioned in passing in the manuscript, as well as on the README included in the source code distribution.
It's not clear whether HULK's performance is due to characteristics of the histogram sketching algorithm or the language of implementation, Go.&lt;/p&gt;
&lt;p&gt;The only hard data offered in the manuscript is a comparison of HULK and &lt;a href="https://github.com/GATB/simka"&gt;Simka&lt;/a&gt;.
HULK positively spanks Simka with an order-of-magnitude improvement in runtime while acheiving comparable (slightly better) accuracy.
Is this a good speed benchmark? I'm not sure.
I wasn't familiar with Simka prior to reading the HULK preprint, although that may reflect a personal deficiency more than anything else.
However, I do think it would have been appropriate to compare against Mash at the very least, as the trend-setter in sketch-based comparisons of metagenomes.&lt;/p&gt;
&lt;h3&gt;Streaming support (incremental sketching)&lt;/h3&gt;
&lt;p&gt;Much is made of histogram sketching being a streaming algorithm both in the original HistoSketch paper and in the HULK preprint.
At first, this didn't seem like a helpful distinction since there is nothing preventing MinHash or CountMin (for example) from sketching data streams just as easily.
When and how to output the sketch is an implementation decision, not a limitation of the algorithm.&lt;/p&gt;
&lt;p&gt;That said, I am unaware of any other sketching tools that output snapshots of a sketch at intermediate intervals while processing the data stream.
This is a Big Deal.
The kind of &lt;em&gt;incremental sketching&lt;/em&gt; HULK supports opens up a new realm of possibilities for dynamic control of the entire pipeline, from sequencing to sketching to analysis.
And unless I'm mistaken, the HULK preprint was also the first bioinformatics study to investigate the accuracy of sketches computed from partial data streams.&lt;/p&gt;
&lt;h3&gt;Robustness for relative data set size&lt;/h3&gt;
&lt;p&gt;There are known issues when using MinHash to compare data sets of significantly different size.
The HULK preprint references the issue twice: one brief mention in the Introduction section...&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;there remain limitations to standard MinHash techniques; such as the loss of k-mer frequency information and the impact of relative set size on Jaccard similarity estimates.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;...and another in the Discussion section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;In particular, it addresses the main limitations of traditional MinHash for certain microbiome analyses.
These being: (i) histogram sketching is not impacted by mismatched set size and (ii), histogram sketching accounts for weighted sets (e.g. k-mer frequency).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This topic is discussed in depth &lt;a href="https://doi.org/10.1101/184150"&gt;in this preprint&lt;/a&gt; and by the Mash authors themselves in &lt;a href="https://genomeinformatics.github.io/mash-screen/"&gt;a blog post&lt;/a&gt;, both posted about a year ago.
HULK claims to be the first tool to solve this problem for (meta)genome analytics.
I don't think this is true...as an unqualified statement at least.
Mash is capable of performing "containment" queries with &lt;code&gt;mash screen&lt;/code&gt; as described in the blog post above.
And while Mash doesn't support similarity calculations for metagenomes of mismatched size, sourmash does with its &lt;code&gt;--scaled&lt;/code&gt; mode.
In both cases, a sampling rate is defined, so a fixed sketch size can no longer be guarantee like it is with HULK.&lt;/p&gt;
&lt;p&gt;How exactly &lt;em&gt;is&lt;/em&gt; HistoSketch robust to differences in relative set sizes? I'm actually not clear on this.
The only discussion of mismatched set sizes I could find in the manuscript are the two shown above.
It's possible there's more information in there for an astute reader who can read between the lines, but I think it would improve the manuscript considerably if the authors clarified this point.&lt;/p&gt;
&lt;p&gt;But even giving the authors the benefit of the doubt, I still think it's a bit of a stretch to claim that HULK is the first tool to address mismatched data set sizes.
I &lt;em&gt;would&lt;/em&gt; fully agree with the sentiment that HistoSketch is the first sketching algorithm that treats comparison of data sets of mismatched size &lt;em&gt;as a primary algorithm design consideration&lt;/em&gt; rather than a bonus feature or alternative mode.
It's admittedly a subtle point, but an important one in my opinion.&lt;/p&gt;
&lt;h3&gt;Sensitivity to &lt;em&gt;k&lt;/em&gt;-mer frequencies&lt;/h3&gt;
&lt;p&gt;The HULK authors claim that standard MinHash tools do not take into account &lt;em&gt;k&lt;/em&gt;-mer frequency information.
I feel like this statement also needs qualification.
It is true that Mash ignores &lt;em&gt;k&lt;/em&gt;-mer abundance completely, but sourmash has supported a &lt;code&gt;--track-abundance&lt;/code&gt; mode for some time.
Assuming all sketches in question were computed with &lt;code&gt;--track-abundance&lt;/code&gt;, the distance calculations with &lt;code&gt;sourmash compare&lt;/code&gt; integrate the &lt;em&gt;k&lt;/em&gt;-mer abundance information.&lt;/p&gt;
&lt;p&gt;However, even in the case of sourmash, &lt;em&gt;k&lt;/em&gt;-mer frequencies don't have any influence on which &lt;em&gt;k&lt;/em&gt;-mers are chosen to occupy the sketch, whereas the &lt;em&gt;Consistent Weighted Sampling&lt;/em&gt; scheme employed in histosketching is sensitive to &lt;em&gt;k&lt;/em&gt;-mer frequencies.
Like the previous point, I'd suggest it's more accurate to say that HistoSketch is novel in including this as a primary design consideration rather than an afterthought.&lt;/p&gt;
&lt;h3&gt;Machine learning integration&lt;/h3&gt;
&lt;p&gt;The authors don't claim that this is the only sketching algorithm that can be integrated with machine learning downstream, but HULK is the first to demonstrate it in the context of bioinformatics.
HULK's support for incremental sketching is also relevant here, as successful classification of a data stream (above some confidence threshold) can terminate processing early.&lt;/p&gt;
&lt;h3&gt;Elegant handling of &lt;em&gt;concept drift&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Concept drift&lt;/em&gt; refers to the phenomenon that occurs when a continuous data stream measures qualitatively different things at different points in time.
By "gradually forgetting" earlier observations, more recent observations can carry more influence and provide a more accurate reflection of what is currently being measured.&lt;/p&gt;
&lt;p&gt;I can't say I fully grok the CWS scheme used by HistoSketch, but the CWS-computed weights do provide a simple and elegant mechanism for handling concept drift in a data stream.
With each new observation, the existing weights are scaled so that they are slightly more likely to be replaced by an updated histogram element.
Over time, these slight adjustments accumulate and older observations are replaced with newer observations.&lt;/p&gt;
&lt;p&gt;So what is the relevance of this to DNA sequence data? I could imagine a few scenarios where this approach might be useful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;arrays of sensors continuously collecting sequence data from different locations to profile soil, marine, or human microbiome composition&lt;/li&gt;
&lt;li&gt;stategically placed sensors continuously monitoring for the presence of pathogens&lt;/li&gt;
&lt;li&gt;a strategy for dynamic control of the DNA sequencer that is sensitive to what has been sequenced "recently" (for some value of "recently")&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each case, changes to a sensor's environment over time would muddle its signal without accounting for shifts in what is being observed.&lt;/p&gt;
&lt;p&gt;I don't think that DNA sequencing technology has quite "made it" yet to the extent needed to make these types of strategy feasible.
And it's hard to estimate when the tech might be ready.
But I think the HULK authors make a great contribution here in helping us imagine what might be possible.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In conclusion, I think HULK and HistoSketch are great contributions to this area of research.
It's notable that HistoSketch accounts for &lt;em&gt;k&lt;/em&gt;-mer frequency and mismatched data set sizes as primary design considerations.
HULK's demonstration that histosketches are suitable substrates for machine learning approaches is also promising.&lt;/p&gt;
&lt;p&gt;I &lt;em&gt;do&lt;/em&gt; think some of the claims the authors make in the preprint are weakly supported or need to be qualified.
But hey, it's a preprint! Hopefully the HULK authors get some constructive feedback from multiple sources so that their final paper is that much stronger.
In the mean time, I'll be watching &lt;a href="https://github.com/will-rowe/hulk"&gt;the HULK repo&lt;/a&gt; closely.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks to &lt;a href="https://twitter.com/ReiterTaylor"&gt;Taylor Reiter (@ReiterTaylor)&lt;/a&gt; whose comments on a draft of this blog post improved it greatly!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;â€¡&lt;/sup&gt;I'm not exactly sure why CountMin is not similarity-preserving, and the authors offer no explanation.
Perhaps it's related to the fact that CountMin retains all data points and MinHash samples a respresentative subset of data points.&lt;/p&gt;</content><category term="sketching"></category><category term="metagenomics"></category><category term="journal club"></category></entry><entry><title>Improvements from applying filters at k-mer counting time in kevlar</title><link href="https://standage.github.io/improvements-from-applying-filters-at-k-mer-counting-time-in-kevlar.html" rel="alternate"></link><published>2018-07-16T00:00:00-04:00</published><updated>2018-07-16T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2018-07-16:/improvements-from-applying-filters-at-k-mer-counting-time-in-kevlar.html</id><summary type="html">&lt;p&gt;One of the fundamental insights of the &lt;a href="https://kevlar.readthedocs.io"&gt;kevlar &lt;em&gt;de novo&lt;/em&gt; variant caller&lt;/a&gt; is the framing of the variant discovery problem as a search for novel k-mers.
In this case, "novel" means abundant in the focal sample and effectively absent from all control samples.
In the early stages of creating kevlar, it quickly became clear that many k-mers satisfying these simple â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the fundamental insights of the &lt;a href="https://kevlar.readthedocs.io"&gt;kevlar &lt;em&gt;de novo&lt;/em&gt; variant caller&lt;/a&gt; is the framing of the variant discovery problem as a search for novel k-mers.
In this case, "novel" means abundant in the focal sample and effectively absent from all control samples.
In the early stages of creating kevlar, it quickly became clear that many k-mers satisfying these simple threshold criteria are not associated with a &lt;em&gt;de novo&lt;/em&gt; variants.
Some come from sample-specific contamination (bacteria, Illumina adapters, etc.), while others derive from alleles present in the reference genome but absent from the controls due to a combination of genetic variation and chance fluctuations in sequencing coverage.
Thus, we introduced filters early on to weed out uninteresting k-mers to focus on those more likely to be associated with &lt;em&gt;de novo&lt;/em&gt; variants.&lt;/p&gt;
&lt;p&gt;Up until very recently, the kevlar workflow applied k-mer filters after the initial pass over the data to find novel k-mers.
Recently I began testing an alternative approach: apply the filters during the initial k-mer counting (enabled by &lt;a href="https://github.com/dib-lab/kevlar/pull/277"&gt;this code update&lt;/a&gt;).
Using this strategy, if a k-mer is present in the reference genome or is contaminant in origin, its abundance is not tracked for later reference.
The direct result is that these k-mers are not declared novel in the first pass over the data, and thus the filters do not need to be applied &lt;em&gt;post facto&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I've noticed two additional secondary benefits to this approach.
First is a modest improvement in kevlar's memory efficiency.
Since k-mers present in the reference genome and in contaminant databases aren't tracked at all, accurate k-mer counts are possible in a smaller amount of memory.
The second is a substantial improvement (&amp;gt;2x decrease) in runtime during the novel k-mer identification step.
I don't know how much of this is due to better cache locality from the smaller k-mer counters versus fewer lookups overallâ€”I'm guessing the latter is the more influential factor.
In any case, this has been the most computationally expensive part of the kevlar workflow overall, and it's encouraging to see improvements of this magnitude.&lt;/p&gt;
&lt;p&gt;Initially I was worried that this approach might require k-mer counts to be recomputed for the final variant likelihood calculations.
However, the likelihood scores are based exclusively on the abundances of the alternate allele k-mers which should not be affected by the filters.
Reference allele k-mers &lt;em&gt;are&lt;/em&gt; considered when computing the error rate, but this only requires abundance/copy number of these k-mers in the reference assembly, not their per-sample abundance.&lt;/p&gt;
&lt;p&gt;In summary, this modest and fairly obvious (in retrospect!) new approach looks like a win-win all around.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of applying k-mer filters in Python land &lt;em&gt;post facto&lt;/em&gt;, they are applied &lt;em&gt;a priori&lt;/em&gt; during k-mer counting using efficient procedures implemented entirely in C++.&lt;/li&gt;
&lt;li&gt;As a result, less memory is needed to get accurate k-mer counts for each sample.&lt;/li&gt;
&lt;li&gt;Also, the task of identifying novel k-mers is sped up substantially, probably due to a decrease in k-mer abundance queries and (to a lesser extent) better cache locality.&lt;/li&gt;
&lt;/ul&gt;</content><category term="kevlar"></category></entry><entry><title>Loading paired reads from position-sorted BAM files</title><link href="https://standage.github.io/loading-paired-reads-from-position-sorted-bam-files.html" rel="alternate"></link><published>2018-06-12T00:00:00-04:00</published><updated>2018-06-12T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2018-06-12:/loading-paired-reads-from-position-sorted-bam-files.html</id><summary type="html">&lt;p&gt;BAM files with sequence alignments sorted by genomic position seem to be the new currency of exchange for large-scale human genome sequencing projects.
This is convenient and practical in many ways for many people.
But in my current research I work a lot with tools that only want/need the sequence information and, for whatever reasons, support only FASTA or â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;BAM files with sequence alignments sorted by genomic position seem to be the new currency of exchange for large-scale human genome sequencing projects.
This is convenient and practical in many ways for many people.
But in my current research I work a lot with tools that only want/need the sequence information and, for whatever reasons, support only FASTA or FASTQ input.&lt;/p&gt;
&lt;p&gt;A quick Google search will identify several freely available tools for extracting read sequences and base qualities from a BAM file and writing them out in FASTQ format.
I have used several of these tools during the last couple of years.
The catch is that if one wants to retain pairing information, all of these tools require the reads in the BAM file to be sorted by read name.
BAMs are more commonly sorted by genomic positionâ€”this is certainly the case for all the data setsÂ I've work withâ€”and for BAMs occupying more than 100G of storage space each, re-sorting requires a non-trivial amount of compute and disk I/O.&lt;/p&gt;
&lt;p&gt;Today as I was waiting yet again for a &lt;code&gt;samtools sort -n&lt;/code&gt; job to finish, I gave this problem some thought.
When read alignments in a BAM file are sorted by genomic position, we should expect paired reads to occur in close proximity.
If this expectation holds true, then position-sorted BAM files lend themselves to a fairly simple and efficient strategy for conversion to paired FASTQ formatâ€”or more generally, to loading read pairsâ€”without the need for sorting by read name.&lt;/p&gt;
&lt;h2&gt;The experiment&lt;/h2&gt;
&lt;p&gt;To assess my intuition, I hastily threw together a little experiment, implemented as the following Python program.
In brief, it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iterates over read alignments in the specified inpyt BAM file&lt;/li&gt;
&lt;li&gt;skips the first million entries (this is probably unnecessary, but it's a habit from working with FASTQ files which often have a bunch of junk reads at the beginning of the file)&lt;/li&gt;
&lt;li&gt;stores the next 10,000 entries in a hashtable (a dictionary, for you Python folks)&lt;/li&gt;
&lt;li&gt;continues iterating over the entries in the BAM file; if the current read is paired with a read already stored in the hashtable, the program calculates the number of entries separating the pair and prints this out&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pysam&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--out&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;reads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;bam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pysam&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AlignmentFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bam&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;storedreads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;pairsfound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;record&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bam&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="n"&gt;readname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;record&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query_name&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;storedreads&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;readname&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;record&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;storedreads&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;readname&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;other_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other_record&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;other_i&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;del&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;pairsfound&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;storedreads&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;processed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;reads&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;record&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;readname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So in the end, this gives a distribution of "distances" between paired-end read alignments in the BAM file, with distance measured as the number of other reads separating the pair.&lt;/p&gt;
&lt;p&gt;I ran this experiment on a BAM file with nearly 1 billion reads and got the following results.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://standage.github.io/images/bam-read-dist.png" alt="Distances between paired read entries in a BAM file" width="750px" /&gt;&lt;/p&gt;
&lt;p&gt;Out of the 10,000 arbitrarily selected reads, 60 mysteriously had no pair anywhere in the entire file.
For another 27, the pair was separated by tens or hundreds of millions of entries, presumably due to a misalignment, and unmapped read, or one of the pairs aligning to an alternate or decoy sequence that sorts to the end of the BAM file.
But for the remaining 99.13% of the reads, pairs were separated only by dozens of entries.&lt;/p&gt;
&lt;h2&gt;The strategy&lt;/h2&gt;
&lt;p&gt;Based on this observation, my proposed strategy is to load BAM entries into memory while iterating through the file.
Assuming the trend observed in my experiment holds throughout the entire file, we can be assured that most BAM entries will not remain in memory long.
Once their paired read has been found (usually within a few dozen entries), both reads can be sent to the output and removed from memory.
If there are any entries left in memory after traversing the entire BAM file, these mateless reads can be output as singletons.&lt;/p&gt;
&lt;p&gt;So just how much memory do we expect this to require?
Let's consider the data set we examined above with about 991 million reads, and let's assume 0.87% of the reads will need to be kept in memory for most or all of the program's runtime.
This amounts to about 8.5 million reads.
In Python, storing this many BAM entries in a Python dictionary required about 9 GB or memory.
This is feasible on many laptops and desktops these days, and is peanuts for the systems running most genomics workflows.
Plus, if this strategy were implemented in C++ or some other language with less overhead than Python, we could expect the memory usage to be even less.&lt;/p&gt;
&lt;h2&gt;The implementation&lt;/h2&gt;
&lt;p&gt;I don't have the attention to implement a high-performance solution for this in C/C++ at the moment, but I'd like to get something down while this is all still fresh in my head.
Throwing together something in Python with the pysam module should be pretty straightforward, so watch this space for updates!&lt;/p&gt;</content><category term="ngs"></category><category term="bam"></category></entry><entry><title>Information content versus data volume and k-mer counting accuracy</title><link href="https://standage.github.io/information-content-versus-data-volume-and-k-mer-counting-accuracy.html" rel="alternate"></link><published>2017-10-06T00:00:00-04:00</published><updated>2017-10-06T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2017-10-06:/information-content-versus-data-volume-and-k-mer-counting-accuracy.html</id><summary type="html">&lt;p&gt;Keeping track of &lt;em&gt;k&lt;/em&gt;-mers for simple operations has become a fundamental component of many bioinformatics techniques.
Two common operations on &lt;em&gt;k&lt;/em&gt;-mers include set membership queries ("is &lt;em&gt;k&lt;/em&gt;-mer &lt;em&gt;X&lt;/em&gt; present in data set &lt;em&gt;Y&lt;/em&gt;?") and abundance queries ("how many times does &lt;em&gt;k&lt;/em&gt;-mer &lt;em&gt;X&lt;/em&gt; occur in data set &lt;em&gt;Y&lt;/em&gt;").
Several probabilistic data structures have been developed to support â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Keeping track of &lt;em&gt;k&lt;/em&gt;-mers for simple operations has become a fundamental component of many bioinformatics techniques.
Two common operations on &lt;em&gt;k&lt;/em&gt;-mers include set membership queries ("is &lt;em&gt;k&lt;/em&gt;-mer &lt;em&gt;X&lt;/em&gt; present in data set &lt;em&gt;Y&lt;/em&gt;?") and abundance queries ("how many times does &lt;em&gt;k&lt;/em&gt;-mer &lt;em&gt;X&lt;/em&gt; occur in data set &lt;em&gt;Y&lt;/em&gt;").
Several probabilistic data structures have been developed to support these types of operations in a memory-efficient manner, and this is still &lt;a href="https://doi.org/10.1093/bioinformatics/btx636"&gt;an active area of research&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my recent work with variant discovery, I make extensive use of the Count-min sketch implementation available from the &lt;a href="http://dx.doi.org/10.21105/joss.00272"&gt;khmer library&lt;/a&gt;.
This data structure is very similar to a Bloom filter, using a fixed amount of memory and multiple hash functions to keep track of &lt;em&gt;k&lt;/em&gt;-mer abundances.
Because the memory is fixed and collisions are left intentionally unresolved, the accuracy of these data structures declines as they approach full capacity.
The precise accuracy of depends on the number of unique &lt;em&gt;k&lt;/em&gt;-mers in the data set being analyzed and the amount of space allocated to the data structure.&lt;/p&gt;
&lt;p&gt;I was recently reminded of the fact that "data does not equal information" in a way that should have been obvious to me from the beginning.
One of the big challenges with doing variant discovery on human genome sized data sets is the sheer volume of data, where whole genome sequencing to about 30x coverage requires more than 100 Gb of raw compressed data per sample.
Accordingly, I am always looking for things I can do to reduce the data to a more manageable size, both to facilitate my own research progress and to make the methods I'm developing more generally useful.&lt;/p&gt;
&lt;p&gt;One of the first ideas I and my colleagues bounced around was to discard all reads that matched the reference genome perfectly before counting &lt;em&gt;k&lt;/em&gt;-mers.
Preliminary results looked deceptively promising, with around 80% of the reads being discarded from each sample.
This certainly reduced the amount of time required to iterate over the reads in subsequent steps of the workflow.
However, this did very little to reduce the &lt;em&gt;information content&lt;/em&gt; of each sample.
Recall that the accuracy of the Count-min sketch is a function of the number of distinct &lt;em&gt;k&lt;/em&gt;-mers and the amount of memory allocated.
Put another way, the only way reduce memory consumption while keeping the accuracy constant is to also reduce the number of distinct &lt;em&gt;k&lt;/em&gt;-mers being stored.&lt;/p&gt;
&lt;p&gt;As it turns out, discarding reads that match the reference genome perfectly does very little to reduce the number of unique &lt;em&gt;k&lt;/em&gt;-mers in a data set.
&lt;em&gt;k&lt;/em&gt;-mers associated with sequencing errors typically outnumber true &lt;em&gt;k&lt;/em&gt;-mers, and this approach does nothing to address these erroneous &lt;em&gt;k&lt;/em&gt;-mers.
And despite discarding such a large proportion of the reads, the majority of the true &lt;em&gt;k&lt;/em&gt;-mers were still present in the remaining reads, many of which we can presume had only a single nucleotide substitution error.&lt;/p&gt;
&lt;p&gt;For a long time I was resistant to the idea of error correction, because of the resources and the additional processing time that this would require.
But had I been thinking in terms of &lt;em&gt;information content&lt;/em&gt; instead of &lt;em&gt;data volume&lt;/em&gt;, this is something I would have investigated much earlier.
Indeed, running the &lt;a href="https://doi.org/10.1186/s13059-014-0509-9"&gt;Lighter&lt;/a&gt; error correction tool on one of my data sets resulted in a 60% reduction in the number of unique &lt;em&gt;k&lt;/em&gt;-mers in each sample.
I was able to cut the memory I had devoted to &lt;em&gt;k&lt;/em&gt;-mer counting in half and achieve even better accuracy than before.
And it turns out my concerns about resources and processing time were unfounded: with 8 threads Lighter required only 90 minutes of runtime and less than 16 GB of RAM, which isn't asking too much of commodity bioinformatics hardware these days.&lt;/p&gt;
&lt;p&gt;Now I need to verify that running error correction doesn't introduce problems later on for variant discovery, but given that error correction tools are usually quite conservative in my experience, I don't anticipate this will be much of a problem.&lt;/p&gt;</content><category term="kevlar"></category><category term="variant discovery"></category><category term="error correction"></category></entry><entry><title>Super simple reverse-complement aware DNA sequence search with rcgrep</title><link href="https://standage.github.io/super-simple-reverse-complement-aware-dna-sequence-search-with-rcgrep.html" rel="alternate"></link><published>2017-03-02T00:00:00-05:00</published><updated>2017-03-02T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2017-03-02:/super-simple-reverse-complement-aware-dna-sequence-search-with-rcgrep.html</id><summary type="html">&lt;p&gt;There are many wonderfully elegant and efficient tools for performing all sorts of exact and inexact searches on large collections of DNA sequences.
Experience has shown, however, that these tools are usually very rigid with respect to their assumptions about input data.
If input files are compressed in a certain way, or stored in a non-standard format, there is usually â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are many wonderfully elegant and efficient tools for performing all sorts of exact and inexact searches on large collections of DNA sequences.
Experience has shown, however, that these tools are usually very rigid with respect to their assumptions about input data.
If input files are compressed in a certain way, or stored in a non-standard format, there is usually some non-trivial work and storage involved in converting the data to a format the software will accept.
There are circumstances in which this extra work is justified, but many times you just need a flexible tool to do a quick-n-dirty search.&lt;/p&gt;
&lt;p&gt;Consider the following, totally not-real never-happened-to-me hypothetical situation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ME&lt;/strong&gt;: &lt;em&gt;I just need to search for a sequence in this text file real quick. I'll use &lt;code&gt;grep&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ALSO ME&lt;/strong&gt;: &lt;em&gt;Oh, the file is gzip-compressed. Use &lt;code&gt;gzgrep&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ME AGAIN&lt;/strong&gt;: &lt;em&gt;Oops, I forgot to search for the sequence's reverse complement as well. Try again.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ME, 5 MINUTES LATER&lt;/strong&gt;: &lt;em&gt;This time I want to search a different file (bzip2-compressed) for 3 sequences and their reverse complements. Invoke &lt;code&gt;bzgrep&lt;/code&gt; with multiple &lt;code&gt;-e&lt;/code&gt; flags.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ME, HEAD ON DESK&lt;/strong&gt;: &lt;em&gt;Uggghh, &lt;code&gt;bzgrep&lt;/code&gt; on Linux doesn't support multiple &lt;code&gt;-e&lt;/code&gt; flags. Pipe output of &lt;code&gt;bzcat&lt;/code&gt; to &lt;code&gt;grep&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Searching for DNA sequences in text files is a very simple task that too often is unnecessarily complicated by uninteresting and frustrating technical details.&lt;/p&gt;
&lt;p&gt;Enter &lt;a href="https://github.com/dib-lab/rcgrep"&gt;rcgrep&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;rcgrep&lt;/strong&gt; is a lightweight wrapper for the UNIX &lt;code&gt;grep&lt;/code&gt; command and is intended to make these irrelevant details disappear as much as possible.
It supports multiple queries, is reverse complement aware, handles mixtures of compressed and uncompressed files automagically, supports streaming, and can tap into &lt;code&gt;grep&lt;/code&gt;'s extended features such as case-insensitive search (&lt;code&gt;grep -i&lt;/code&gt;) and before- and after-context reporting (&lt;code&gt;grep -B&lt;/code&gt; and &lt;code&gt;grep -A&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This tool, like a few others I've artisanally hand-crafted over the past few years, is an example of &lt;em&gt;frustration-driven development&lt;/em&gt;.
It's such a simple and obvious idea that I'm sure it's been implemented and re-implemented in dozens of ways by hundreds of researchers over the years.
I hope that the small amount of work I've put into publishing this tool and making it easy to install and use will save the field a few hundred computational-biologist-hours: time much better spent on doing some awesome science.&lt;/p&gt;
&lt;p&gt;To install, invoke &lt;code&gt;pip install rcgrep&lt;/code&gt; or download directly from the GitHub repository at &lt;a href="https://github.com/dib-lab/rcgrep"&gt;https://github.com/dib-lab/rcgrep&lt;/a&gt;.&lt;/p&gt;</content><category term="software"></category><category term="rcgrep"></category><category term="frustration driven development"></category></entry><entry><title>Streaming data from the SRA with fastq-dump</title><link href="https://standage.github.io/streaming-data-from-the-sra-with-fastq-dump.html" rel="alternate"></link><published>2017-01-24T00:00:00-05:00</published><updated>2017-01-24T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2017-01-24:/streaming-data-from-the-sra-with-fastq-dump.html</id><summary type="html">&lt;p&gt;NCBI's &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Sequence Read Archive&lt;/a&gt; is the go-to repository for published genome-scale sequence data sets.
Although there are a variety of ways to download sequence data from SRA, the &lt;code&gt;fastq-dump&lt;/code&gt; command from the &lt;a href="https://www.ncbi.nlm.nih.gov/Traces/sra/?view=software"&gt;SRA Toolkit&lt;/a&gt; is the most convenient in my opinion.
In fact, with a few settings tweaks &lt;code&gt;fastq-dump&lt;/code&gt; can stream data directly from the SRA into an analysis pipeline â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;NCBI's &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Sequence Read Archive&lt;/a&gt; is the go-to repository for published genome-scale sequence data sets.
Although there are a variety of ways to download sequence data from SRA, the &lt;code&gt;fastq-dump&lt;/code&gt; command from the &lt;a href="https://www.ncbi.nlm.nih.gov/Traces/sra/?view=software"&gt;SRA Toolkit&lt;/a&gt; is the most convenient in my opinion.
In fact, with a few settings tweaks &lt;code&gt;fastq-dump&lt;/code&gt; can stream data directly from the SRA into an analysis pipeline.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For a true streaming approach, you'll want to disable local file caching with &lt;a href="https://github.com/ncbi/sra-tools/wiki/Toolkit-Configuration"&gt;vdb-config&lt;/a&gt;.
   Especially on clusters with tight quotas on home directory storage, the default settings can be very problematic.&lt;/li&gt;
&lt;li&gt;If you have paired reads, use the &lt;code&gt;--split-files&lt;/code&gt; flag for proper printing of pairs and the &lt;code&gt;--stdout&lt;/code&gt; flag (or &lt;code&gt;-Z&lt;/code&gt; for short) so that the data is printed in &lt;em&gt;interleaved Fastq&lt;/em&gt; format, rather than in two paired files (as is the default).&lt;/li&gt;
&lt;li&gt;By default, the read IDs returned by &lt;code&gt;fastq-dump&lt;/code&gt; don't include any pairing information, which some programs rely on for processing paired-end data.
   Include the options &lt;code&gt;--defline-seq '@$ac.$si.$sg/$ri' --defline-qual '+'&lt;/code&gt; to append a &lt;code&gt;/1&lt;/code&gt; or &lt;code&gt;/2&lt;/code&gt; to the end of each read ID for pairing information, and to throw away all of the superfluous and redundant info in the 3rd line of each Fastq record.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following example pipes the SRA data set with the accession ERR612477 into a processing pipeline.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fastq-dump --split-files --defline-seq &amp;#39;@$ac.$si.$sg/$ri&amp;#39; --defline-qual &amp;#39;+&amp;#39; -Z ERR612477 \
    | trim-low-abund.py --ksize 25 --max-memory-usage 2G --variable-coverage - \
    | my-favorite-mapper-or-assembler &amp;gt; out.dat
&lt;/pre&gt;&lt;/div&gt;</content><category term="sra"></category><category term="streaming"></category><category term="ngs"></category></entry><entry><title>Thoughts on research software from the PSRN workshop</title><link href="https://standage.github.io/thoughts-on-research-software-from-the-psrn-workshop.html" rel="alternate"></link><published>2016-10-20T09:30:00-04:00</published><updated>2016-10-20T09:30:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-10-20:/thoughts-on-research-software-from-the-psrn-workshop.html</id><summary type="html">&lt;p&gt;In which I ramble on about experimental science, research software, and cyberinfrastructure engineering.&lt;/p&gt;</summary><content type="html">&lt;p&gt;*&lt;em&gt;In which I ramble on about experimental science, research software, and cyberinfrastructure engineering.&lt;/em&gt;*&lt;/p&gt;
&lt;p&gt;I'm on my way home from a workshop on cyberinfrastructure and postgraduate training hosted by the &lt;a href="http://bti.cornell.edu/news/plant-science-research-network-launches/"&gt;Plant Science Research Network&lt;/a&gt;.
I, like all the other participants, had some assigned prep work prior to the workshop, which got me thinking (&lt;a href="https://standage.github.io/misconceptions-about-research-software.html"&gt;again&lt;/a&gt;) about research software.
The purpose of this post is to summarize my thoughts while they're fresh in my head.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To be clear, the contents of this post represent my personal perspectives, and I do not imply that any of these perspectives are endorsed by the PSRN or any institution with which I'm affiliated.
I do hope, however, that they turn out to be useful.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Several days in advance, the workshop organizers sent out some material to introduce the subject matter and prime everyone for productive discourse.
Each of us was assigned to one of four extreme scenarios regarding resource availability (high vs. low) and research motivation ("advancement" / curiosity-driven / proactive vs. "necessity" / problem-solving / reactive), and then asked to consider the plant science community's opportunities and challenges with respect to cyberinfrastructure and postgraduate training in the given scenario.
Specifically, we were asked to come to the workshop ready to share an "early indicator" that would suggest a trend in the direction of our assigned scenario.
I've summarized my overall experience at the workshop &lt;a href="https://standage.github.io/my-thoughts-on-the-psrn-workshop-on-cyberinfrastructure-and-training.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Initial concern&lt;/h2&gt;
&lt;p&gt;The prep material referred quite a bit to software, but it was almost exclusively mentioned&lt;sup&gt;1&lt;/sup&gt; in the context of cyberinfrastructure components integrationâ€”in other words, very low-level systems engineering.
One of my initial reactions was that the prep material failed to acknowledge the wider spectrum of research software that exists, with systems engineering at one extreme and very domain-specific (and sometimes study-specific) research codes at the other extreme.
The former requires deep engineering expertise with some minimal exposure to plant science domain knowledge, while the latter requires extensive plant science expertise with minimal (perhaps even self-taught) programming experience.
But there is a lot of middle ground there, with most research software closer to the "domain expertise" extreme.
And it doesn't do any good to lump everything together under one umbrella.
This too often results in software being designated as second-class, with plant science (or another life science discipline, more generally) being elevated as the "real" science and software demoted to a purely technical exercise best delegated to trainees (or technicians, if you're fortunate enough to afford them) or outsourced to engineers for heavy technical lifting.&lt;/p&gt;
&lt;h2&gt;Pleasant surprise&lt;/h2&gt;
&lt;p&gt;Despite my concern coming into the workshop, I must say that I was pleasantly surprised with the discussions around software.
This point was acknowledged throughout the discussion, and it was quite well received, and even connected to other relevant concerns and takeaways.
I'm not sure I can even take credit for this, since I don't remember if I was the only or even the first person to raise the issue.
I'm cautiously optimistic that the plant science community can foster continued dialog and perhaps even (crossed fingers!!!) catalyze some actual cultural change in this regard.&lt;/p&gt;
&lt;h2&gt;Stepping back: research software and experiments&lt;/h2&gt;
&lt;p&gt;All of this has me &lt;a href="https://standage.github.io/misconceptions-about-research-software.html"&gt;thinking again&lt;/a&gt; about &lt;em&gt;why&lt;/em&gt; research software is rarely considered a legitimate intellectual contribution to science.&lt;sup&gt;2&lt;/sup&gt;
As I've said before, I think it's important to compare software to experiments.
Which components of experiments are valued as legitimate intellectual research activities?
Of course you must include the design of the experiment, the evaluation of the experimental results, and the interpretation and dissemination of the findings.
The actual execution of the experiment, on the other hand, is a purely technical exercise.
An important one no doubt, but something that could be delegated to a competent technician with minimal domain expertise.
The keeping of a detailed and accurate lab notebook could be argued either way, but is probably more on the technical side than the intellectual.&lt;/p&gt;
&lt;p&gt;So how does this translate to software?
I will again echo &lt;a href="http://phdops.kblin.org/software-dev-intellectual-contribution.html"&gt;Kai Blin's argument&lt;/a&gt; that designing research software is like designing experiments, and just as with experiments, you must evaluate, interpret, and disseminate computational results.
These are all inherently intellectual activities.
Ideally, with some upfront design, &lt;em&gt;implementing&lt;/em&gt; the software and then &lt;em&gt;executing&lt;/em&gt; it should be a fairly straightforward technical exercise, also perfectly fine to designate to competent technicians with minimal domain expertise as resources allow.
But research software is so much more than just "writing code", and should definitely not be treated the same as code for systems-level cyberinfrastructure engineering.&lt;/p&gt;
&lt;h2&gt;An ideal world&lt;/h2&gt;
&lt;p&gt;In an ideal world, design is completely separated from execution.
Entire experiments are thoroughly designed and critically evaluated before a single sample is collected or reagent is touched.
Consideration is given to what controls are in place to validate assumptions and verify experimental integrity, to how the results will be evaluated, and to which statistics and data visualizations will be most interesting to reportâ€”regardless of the result!
Once all this is fleshed out, conducting the experiment is almost mechanical, with little or no critical thinking required.
Collecting, formatting, interpreting, and disseminating the results is not quite so formulaic, but the critical thinking required at this stage has been well primed by the upfront design.&lt;/p&gt;
&lt;p&gt;The ideal world of research software looks very similar.
The software is thoroughly spec'd out before a single line of code is written.
Consideration is given to what automated tests are in place to validate assumptions and verify computational accuracy, to how the results will be evaluated, and to which statistics and data visualizations will be most interesting to report.
Once the design is critically vetted, implementing the software is almost mechanical, with little or no critical thinking required.&lt;sup&gt;3&lt;/sup&gt;
Collecting, formatting, interpreting, and disseminating the results is not quite so formulaic, but the critical thinking required at this stage has been well primed by the upfront design.&lt;/p&gt;
&lt;h2&gt;We do not live in that world&lt;/h2&gt;
&lt;p&gt;I have enough exposure to experimental molecular biology to know that these ideals are rarely met in experimental science.
However, my experience allows me to speak in more depth about why these ideals are rarely met in computational science.&lt;/p&gt;
&lt;p&gt;Part of the problem is that as time- and resource-strapped scientists, often poorly trained with respect to software, we rarely invest sufficient time in the upfront design.
As a result, we often end up taking the less than optimal approach of design-by-implementation, which can lead to messy and inflexible and inaccurate software.
Perhaps now is a good time to reiterate the fact that this workshop focused on both cyberinfrastructure &lt;em&gt;and training&lt;/em&gt;.
Training won't necessarily help with time and resource limitations, but it can certainly expose a nacent plant-scientist-programmer to some basic software development principles that help avoid common pitfalls.&lt;/p&gt;
&lt;p&gt;The other side of this coin is that software requirements are often hard to specify for cutting-edge research.
The problem spaces are typically underdefined and rife with subtle complexity, making it difficult to clearly articulate solid software requirements before some initial data exploration, which itself requires some software/computing literacy.
It often makes sense, therefore, to adopt an iterative approach to research software development: building a rudimentary prototype, evaluating its behavior/accuracy/utility, refining the design requirements, and then repeating.
Again, a minimal amount of training with basic software development practices can make a huge difference in the scientist's effectiveness and morale, as well as the desired research outcomes.&lt;/p&gt;
&lt;h2&gt;Reusable software libraries&lt;/h2&gt;
&lt;p&gt;Taking things a step further, it's important to note that research software can be a powerful platform in which to explore and evaluate conceptual models.
One of the discussion points that came up in the workshop is that building and reusing software libraries is not incentivized in the current plant science (and wider science) culture.
All of the professional incentives push scientists toward implementing new ideas from scratch (and getting a paper out of it, which can then be cited), rather than building on existing tools (nobody will publish or credit contributions like this).&lt;/p&gt;
&lt;p&gt;Building on existing code requires more attention to software engineering than is realistic to invest in the current funding/career/credit landscape.
Imagine a scientist interested in exploring the nuances of transcriptome assembly.
The most common scenarios in the current climate involve the scientist either 1) losing their sanity trying to understand and extend existing assembly software that the original scientist was never supported to clean up, organize, and document, or 2) building the assembler software components (de Bruijn graph implementation, assembly algorithm, etc.) from scratch.
This is a genomics example (that's my research interest), but the principle can apply to research software in any area of plant research.&lt;/p&gt;
&lt;p&gt;Those that produce good software in today's academic environment do so &lt;em&gt;in spite&lt;/em&gt; of the system and at great personal sacrifice, not &lt;em&gt;because&lt;/em&gt; the system supports them in doing so.
Not all research software warrants this additional attentionâ€”there is definitely a place for &lt;a href="http://ivory.idyll.org/blog/2015-how-should-we-think-about-research-software.html"&gt;"single-use" research software&lt;/a&gt;â€”but there is a critical need in some cases to support development of reusable software libraries.&lt;sup&gt;4&lt;/sup&gt;
The plant science community has a huge opportunity to address this with relevant career support and incentives.&lt;/p&gt;
&lt;h2&gt;Closing thoughts&lt;/h2&gt;
&lt;p&gt;I'm encouraged that the diverse plant science community is beginning to acknowledge that software is a critical component of the plant research enterprise, not only for what it brings to the table in a technical sense but also what it represents in an intellectual sense.
Transforming this understanding into cultural change is a much bigger challenge, but one I hope this community is willing to fight for.&lt;/p&gt;
&lt;p&gt;And if my lengthy ramblings have not yet diverted your attention to more important matters, I salute you!
Thanks for humoring me!
More importantly, I'm interested in your feedback.
What do you think?
Did I miss an important point or get something completely wrong?
Leave a note in the comments if you feel so inclined.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;It's possible that I just perceived it that way, but the language and context seemed skewed towards cyberinfrastructure engineering.&lt;br&gt;
&lt;sup&gt;2&lt;/sup&gt;I think we're at the point that many people have recognized this as a problem, but the incentive structures and culture have not yet changed to reflect this.&lt;br&gt;
&lt;sup&gt;3&lt;/sup&gt;This is only partially true. Sometimes multiple implementation approaches exist and tradeoffs must be considered, but really these are technical considerations rather than scientific considerations. Of course, the same can be said of lab techniques...&lt;br&gt;
&lt;sup&gt;4&lt;/sup&gt;This is quickly becoming a complex multi-dimensional space. We've already discussed the "domain" axis (cyberinfrastructure engineering &amp;lt;---&amp;gt; highly specialized plant science analysis) and the "re-use" axis (libraries &amp;lt;---&amp;gt; single-use proof-of-concept software), but another axis that was brought up was the "scale and integration" axis (research &amp;lt;---&amp;gt; production). The cutting edge of plant science insights happens at the small-scale research level, whereas the integration of critical cyberinfrastructure components requires production-scale strategies.&lt;/p&gt;</content><category term="software"></category><category term="science"></category><category term="cyberinfrastructure"></category></entry><entry><title>My thoughts on the PSRN workshop on cyberinfrastructure and training</title><link href="https://standage.github.io/my-thoughts-on-the-psrn-workshop-on-cyberinfrastructure-and-training.html" rel="alternate"></link><published>2016-10-20T09:00:00-04:00</published><updated>2016-10-20T09:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-10-20:/my-thoughts-on-the-psrn-workshop-on-cyberinfrastructure-and-training.html</id><summary type="html">&lt;p&gt;I am on my way home from a workshop on cyberinfrastructure and postgraduate training hosted by the &lt;a href="http://bti.cornell.edu/news/plant-science-research-network-launches/"&gt;Plant Science Research Network&lt;/a&gt;.
It was a grueling three-day sprint, but it brought together a phenomenal diversity of experience and perspectives on the relevant issues.
I wanted to summarize my thoughts while they are still fresh in my head.
Beware: what follows is â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am on my way home from a workshop on cyberinfrastructure and postgraduate training hosted by the &lt;a href="http://bti.cornell.edu/news/plant-science-research-network-launches/"&gt;Plant Science Research Network&lt;/a&gt;.
It was a grueling three-day sprint, but it brought together a phenomenal diversity of experience and perspectives on the relevant issues.
I wanted to summarize my thoughts while they are still fresh in my head.
Beware: what follows is a bit of a brain dump!&lt;/p&gt;
&lt;h2&gt;Why me?&lt;/h2&gt;
&lt;p&gt;&lt;img alt="What are you doing here?" src="https://standage.github.io/images/whatareyoudoinghere.gif"&gt;&lt;/p&gt;
&lt;p&gt;Those who know me best may be wondering what I was doing at a &lt;strong&gt;&lt;em&gt;plant&lt;/em&gt;&lt;/strong&gt; science workshop.
That question certainly came up several times as I explained my graduate work (insect genomics) and postdoctoral work (rare genomic variant discovery in human) to other workshop attendees.
The long-winded answer is that in addition to working in a plant genetics lab as an undergraduate before joining a plant genomics lab as a new Ph.D. student (even though I went on to study mostly insects!), I've also been an active science cyberinfrastructure user, enthusiast, advocate, and instructor for many years.
But the &lt;em&gt;more honest&lt;/em&gt; answer is that somebody much more influential than me was invited first, but was unable to attend, and recommended me in their place. :-)&lt;/p&gt;
&lt;h2&gt;Layout&lt;/h2&gt;
&lt;p&gt;The workshop organizers sent out some material and made assignments several days before the workshop.
As an early career scientist and a relative outsider to the plant science community, I was already skeptical that I would be able to contribute meaningfully to the discourse of the workshop.
And as I started reviewing these workshop prep materials, I became even more skeptical that the "corporate strategy" they were describing would play out well for rooms full of academics.&lt;sup&gt;1&lt;/sup&gt;
It turns out that I enjoyed the workshop immensely, and I think the layout helped make it a worthwhile experience for everyone involved.&lt;/p&gt;
&lt;p&gt;The workshop was organized as a &lt;a href="https://en.wikipedia.org/wiki/Scenario_planning"&gt;scenario planning&lt;/a&gt; session, which (based on feedback from earlier planning efforts) was focused on two critical uncertainties regarding the future of plant research.
The first is &lt;strong&gt;resource availability&lt;/strong&gt;, the question of whether funding and other resources will be &lt;em&gt;abundant&lt;/em&gt; or &lt;em&gt;limited&lt;/em&gt;.
The second uncertainty is the primary &lt;strong&gt;driver of research&lt;/strong&gt;, the question of whether the plant science community is motivated primarily by &lt;em&gt;necessity and problem solving&lt;/em&gt; (reactive) or by &lt;em&gt;advancement and curiosity&lt;/em&gt; (proactive).
These uncertainties served as two axes for brainstorming alternative hypothetical futures: the axes form four quadrants which served as scenarios for our discussion.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;4                           Abundant                                1
                               ^
                               |
                               |
       (Resource availability) |
                               |
                               |
                               |
Necessity &amp;lt;-----------------------(Driver of research)--&amp;gt; Advancement
                               |
                               |
                               |
                               |
                               |
                               |
                               v
3                           Limited                                 2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Prior to the workshop, participants were assigned to one of the four scenarios and asked to reflect on what (plant) science would be like if that scenario were taken to the extreme.
Once gathered at the workshop, we then discussed opportunities and challenges related to postgraduate training and cyberinfrastructure in the assigned hypothetical scenario, as well as recommendations we would make for immediate action if the scenario was to be our inevitable and unavoidable future.&lt;/p&gt;
&lt;p&gt;One could argue that the organizers could have chosen different or better axes.
Perhaps this is true.
But the idea was to anchor the discussion and nail down some givens so that each working group could explore the remainder of the uncertainty spaceâ€”and associated opportunities and challengesâ€”in more detail.
I think the scenario planning strategy effectively satisfied this purpose.&lt;/p&gt;
&lt;h2&gt;Recommendations&lt;/h2&gt;
&lt;p&gt;Of course, the actual future is not going to be any one of those individual scenarios, but some composite of all four.
So after we considered each scenario in separate groups, we all consolidated again to share recommendations and look for common ground.
Despite the very divergent scenarios, we did indeed find some recurring themes in the recommendations made by each of the working groups.
These will no doubt be covered in detail in the official post-workshop report, so I won't attempt to make a comprehensive list here.&lt;/p&gt;
&lt;p&gt;I will say that &lt;strong&gt;connections&lt;/strong&gt; were a theme that came up over and over again: collaboration between scientists, integration of distributed data and tools, and sincere engagement of all relevant stakeholders, especially legislature and the general public.
The fundamental need to address &lt;strong&gt;training in data and computing literacy&lt;/strong&gt; (distinct from computer science!) &lt;strong&gt;throughout all career stages&lt;/strong&gt;â€”even (and especially) as early as high schoolâ€”was another persisent recommendation.
Several plugs were made for &lt;a href="http://software-carpentry.org/"&gt;Software Carpentry&lt;/a&gt; and &lt;a href="http://www.datacarpentry.org/"&gt;Data Carpentry&lt;/a&gt; as an excellent model for such training.&lt;/p&gt;
&lt;p&gt;The group also emphasized the detrimental effect of some of the plant science community's (and the wider science community's) legacy practices/culture, such as the general resistance to sharing and openness, the lack of incentives for making code/data/metadata documented/reusable, the general neglect towards sufficient standards development, and the reticence of academics toward valuing diverse career paths in (plant) science.&lt;/p&gt;
&lt;h2&gt;The &lt;strike&gt;end&lt;/strike&gt; beginning&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Not Bad Obama" src="https://standage.github.io/images/notbad.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Whoa, that's a lot!
And actually &lt;em&gt;addressing&lt;/em&gt; all these issues and recommendations in any meaningful way is going to be an extremely ambitious endeavor, no doubt about it.
But I'm pretty impressed that this diverse group was able to converge so well on a shared vision.
I credit the organizers that went to great lengths to include scientists from diverse career stages, training backgrounds, and industries.
By putting us together in small groups with both early career scientists and soon-to-be emeritus full professors, both acadmic and industry scientists, both plant science specialists and cyberinfrastructure specialists, we were all exposed to a broad variety of perspectives and able to contribute to a critical and constructive dialogue.
My initial misgivings about not being able to contribute were, I'm happy to report, entirely misplaced!&lt;/p&gt;
&lt;p&gt;Kudos to the workshop's organizers and participants for making this a great event.
I look forward to seeing (and where I can, contributing to) great things in the plant science community in the near future!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;The workshop organizers also did a good job including interested stakeholders from industry and government, but I wasn't aware of this coming in to the workshop.&lt;/p&gt;</content><category term="education"></category><category term="academics"></category><category term="cyberinfrastructure"></category></entry><entry><title>An idiot's guide to loading reads from a BAM file</title><link href="https://standage.github.io/an-idiots-guide-to-loading-reads-from-a-bam-file.html" rel="alternate"></link><published>2016-09-08T00:00:00-04:00</published><updated>2016-09-08T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-09-08:/an-idiots-guide-to-loading-reads-from-a-bam-file.html</id><summary type="html">&lt;p&gt;tl;dr? It's fine, just ignore secondary/supplementary alignments and don't disable reporting of unaligned reads.&lt;/p&gt;</summary><content type="html">&lt;p&gt;tl;dr? It's fine, just ignore secondary/supplementary alignments and don't disable reporting of unaligned reads.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I recently started my first &lt;em&gt;real&lt;/em&gt; foray into human genomics, and if I thought I was overwhelmed with data before, I'm much more so now.
(Thank goodness I'm not &lt;strike&gt;stupid&lt;/strike&gt; ambitious enough to work on marine or soil metagenomics!)
One challenge I've encountered already is that this field seems to be moving toward BAM files&lt;sup&gt;1&lt;/sup&gt; for distributing not only read alignments against a reference genome but also the original read sequences themselves.
I have a passing familiarity with SAM/BAM files, but (barring a few exceptions) I've always treated them as a black box before, using "trusted" tools to create and process them.
Now that I'm faced with the prospect of writing software to take them as input, I've really had to bite the bullet and dig into the nitty gritty.&lt;/p&gt;
&lt;h2&gt;BAM is the new Fastq&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/FASTQ_format"&gt;Fastq format&lt;/a&gt; has long been the standard for storing nucleotide sequence reads from genome-scale sequencing platforms such as Illumina.
Although it is riddled with the &lt;a href="https://www.biostars.org/p/7126/#7136"&gt;same issues as essentially every other bioinformatics data format&lt;/a&gt;â€”a redundant and incomplete spec (&lt;em&gt;What *is* the purpose of the third line? Can the sequence be split across multiple lines?&lt;/em&gt;) and inconsistent usage (&lt;em&gt;Which quality encoding was used? Are pairs interleaved or split into different files?&lt;/em&gt;)â€”the Fastq format's stripped-down simplicity is no doubt a big factor in its wide adoption.
On the other hand, the &lt;a href="https://en.wikipedia.org/wiki/SAM_(file_format)"&gt;SAM format&lt;/a&gt; was designed for storing read alignments against a reference, with BAM as its compressed binary counterpart.
When it comes to analyzing the raw reads produced by a sequencer, Fastq is everybody's natural first choice.&lt;/p&gt;
&lt;p&gt;The scale of human genomics data collection is proving challenging, however.
When a single modest-coverage sample requires on the order of 100 GB for storage, and you're sequencing thousands of samples, you don't want to store duplicates of your data.
And since the majority of scientists in the field are only interested in the alignments anyway, it makes much more sense to publish and archive only BAM files rather than only Fastq files.&lt;/p&gt;
&lt;h2&gt;But what if I just want the reads?&lt;/h2&gt;
&lt;p&gt;So the big question then is: if I'm not interested in the alignments and only want the original read sequences, can I get these from the BAM file?
Those more familiar with SAM/BAM could probably have given an immediate answer, but as a relative n00b I had several questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the sequence reported in the BAM file the read sequence, the reference sequence, or an alignment artifact (like a consensus)?&lt;/li&gt;
&lt;li&gt;Do BAM files include unmapped reads?&lt;/li&gt;
&lt;li&gt;Are multi-mapped reads reported multiple times?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, is it possible to extract all input read sequences non-redundantly from a BAM file?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(I should note at this point that I wasn't &lt;strong&gt;completely&lt;/strong&gt; unaware that BAM-to-Fastq converters exist&lt;sup&gt;2&lt;/sup&gt;, so I probably should have approached this question with a bit more confidence.)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Some probing&lt;/h2&gt;
&lt;p&gt;To satisfy my curiousity, I did a bit of detective work on the BAM files I'm analyzing in my current project.
Since the files are huge (&amp;gt;100GB per) and stored on a busy cluster, I selected only the first 5 million-ish reads from the file for testing (processing the whole file would have required submitting a job request to the queue).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;samtools view -h mysample.bam \
    | head -n 5000000 \
    | samtools view -b -o mysample.1st5mil.bam
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Checking whether the file contains unmapped reads is quite simple.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;samtools view -f 4 -c mysample.1st5mil.bam
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Checking whether multimapped reads are reported multiple times took a bit more command-line fu.
The following command cuts out the read ID from each alignment, sorts, reports the counts of each uniq read ID, sorts again by counts, and then reports the top 10 reads by count.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;samtools view mysample.1st5mil.bam \
    | cut -f 1 \
    | sort \
    | uniq -c \
    | sort -rn \
    | head -n 10
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;All of the top 10 read IDs occurred four times in the input.
I suspected that two times would simply mean both the read and its pair were found, but four times means that both the read and its pair were each reported twice.
To confirm, I searched the file for several of the duplicated read IDs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;samtools view mysample.1st5mil.bam | grep ReadIdHere
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sure enough, in each case, both of the read pairs were reported once in full, and a second time as a hard-clipped secondary alignment (256 or 0x100 in the bitwise flag).
The SAM specification is a bit sparse on when hard and soft clipping are intended to be used, but the point is moot since these are marked as secondary alignments.&lt;/p&gt;
&lt;p&gt;I then used the following commands to ensure that all hard-clipped alignments are marked as secondary (and that all secondary alignments are hard-clipped).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;samtools view -f 256 -c mysample.1st5mil.bam
samtools view -f 256 mysample.1st5mil.bam | cut -f 6 | grep -c H
samtools view -F 256 mysample.1st5mil.bam | cut -f 6 | grep -c H
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So far so good.
The only point remaining is to verify that the sequences in the BAM file actually correspond to the read and not the reference or some kind of alignment consensus.
Without access to the original reads, I could only refer back to the SAM specification, which describes the 10th column of an alignment record as the "segment sequence".
Elsewhere in the SAM spec, it is said a read may be composed of multiple segments, so for primary alignments I think we're safe in saying the reported sequence is the entire original read sequence.&lt;/p&gt;
&lt;h2&gt;A little test&lt;/h2&gt;
&lt;p&gt;Based on this detective work, it looks like BAM files do indeed contain the information we need to pull out the original read sequence.&lt;sup&gt;3&lt;/sup&gt;
On one hand, it's not too surprising: this kind of thing has likely come up as authors of the SAM spec and mapping software refined their tools over time.
On the other hand, I couldn't shake the feeling that, as a newcomer, there might be some problematic detail that I've missed.
I concluded that the only way to ease my unsettled mind was to do a test where I knew the (Fastq) input, and could verify that the (BAM) output had exactly the information I needed.&lt;/p&gt;
&lt;p&gt;Here is the basic outline of the test.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grab a couple of chromosomes from the human reference genome (commands not shown)&lt;/li&gt;
&lt;li&gt;grab a small sample (â‰ˆ1 million reads) from a Fastq file I have access to (commands not shown)&lt;/li&gt;
&lt;li&gt;index the reference sequence for mapping&lt;/li&gt;
&lt;li&gt;map the reads to the reference, produce BAM file&lt;/li&gt;
&lt;li&gt;extract read sequences from the BAM file, compare to contents of Fastq file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I decided to test both BWA and Bowtie2 with default settings, figuring that &amp;gt;95% of human genome stuff is going to involve data mapped by of these two tools.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bwa index hg.fasta
bwa mem -t 4 hg.fasta reads-subset.fastq | samtools view -b -o test.bwa.bam

bowtie2-build hg.fasta hg.fasta.bt2
bowtie2 -p 4 -x hg.fasta.bt2 -U reads-subset.fastq | samtools view -b -o test.bt2.bam
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now let's extract all of the sequences corresponding to primary alignmentsâ€”not marked as secondary (256 or 0x100) or supplementary (2048 or 0x800)â€”from each BAM file.
If we sort the sequences, they should be identical right?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# 2048 + 256 = 2304
samtools view -F 2304 test.bwa.bam \
    | cut -f 10 \
    | sort \
    &amp;gt; test.bwa.seq.txt
samtools view -F 2304 test.bt2.bam \
    | cut -f 10 \
    | sort \
    &amp;gt; test.bt2.seq.txt
diff -q test.bwa.seq.txt test.bt2.seq.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ruh, roh!
It looks like they're not the same!
What's going on?
Both files contain exactly 1,000,000 lines, as expected.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wc -l test.bwa.seq.txt test.bt2.seq.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;How many are shared between them?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;comm -12 test.bwa.seq.txt test.bt2.seq.txt | wc -l
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It looks like only about 84% are shared in this case.
After a bit more investigation, I discovered that in some cases the mapper will report the reverse complement of the read sequence instead of the original read sequence.
This of course is not at all problematic for BAM paring in general, but it complicated my rudimentary evaluation using the &lt;code&gt;diff&lt;/code&gt; command.
To this end, I wrote a simple C++ program that would print out each sequence it was given &lt;em&gt;as well as its reverse complement&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;algorithm&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;

&lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="nf"&gt;comp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;A&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;T&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;T&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;A&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;C&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;G&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;G&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;C&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;revcomp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;getline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;revcomp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rbegin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rend&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;revcomp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;comp&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;revcomp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;revcomp&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;revcomp&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="sc"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can now use this to confirm that the â‰ˆ16% of sequences that don't match between BWA and Bowtie are simply reverse complements of each other.
If we compile this program and re-run the pipeline with it included, we should get identical output.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;g++ -Wall -O3 --std=c++11 -o rc rc.cpp
samtools view -F 2304 test.bwa.bam \
    | cut -f 10 \
    | ./rc \
    | sort \
    &amp;gt; test.bwa.seq.txt
samtools view -F 2304 test.bt2.bam \
    | cut -f 10 \
    | ./rc \
    | sort \
    &amp;gt; test.bt2.seq.txt
diff -q test.bwa.seq.txt test.bt2.seq.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Woohoo, success!
Of course, this only checks that the mappers give identical results.
What we're &lt;em&gt;really&lt;/em&gt; interested in is whether they both match the contents of the original Fastq file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;paste - - - - &amp;lt; reads-subset.fastq \
    | cut -f 2 \
    | ./rc \
    | sort \
    &amp;gt; test.fastq.seq.txt
diff -q test.fastq.seq.txt test.bwa.seq.txt
diff -q test.fastq.seq.txt test.bt2.seq.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is indeed a match, and I think I can confidently mark this case closed.&lt;/p&gt;
&lt;h2&gt;Epilogue&lt;/h2&gt;
&lt;p&gt;I &lt;em&gt;did&lt;/em&gt; warn you in the title that this was an idiot's guide, right?
A lot of this is probably very elementary stuff to guys and gals that work with mappers and BAM file internals on a daily basis.
Even so, I hope I've demonstrated clearly and comprehensively that even if you're interested only in the original read sequence, BAM files are a suitable data format.
Of course...there are some caveats:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Caveats" src="https://standage.github.io/images/genie.gif"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some mappers give you the option to not report unmapped reads.
  Enabling these settings is problematic if the Fastq files are not published alongside the BAM files.&lt;/li&gt;
&lt;li&gt;Some mappers may behave differently than the two I evaluated, which (although unlikely) might render some points of my assessment invalid.&lt;/li&gt;
&lt;li&gt;Performing quality control prior to mapping is convenient for those using the alignments, but prevents users from ever going back and applying a different (improved?) quality control procedure&lt;sup&gt;4&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than that, it looks like you're just fine grabbing the segment sequence from each alignment except those marked as secondary or supplementary.&lt;/p&gt;
&lt;p&gt;But let me open this up: is there anything I've missed, any potential pitfalls that I've overlooked?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A big thanks to Carrie Ganote, Ali Berens, Titus Brown, and Fereydoun Hormozdiari for discussions on the topic and feedback on a draft of this blog post.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Or, in the longer term, CRAM files, which are different but supposedly backwards compatible with BAM.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;a href="http://seqanswers.com/forums/showthread.php?t=7061"&gt;This thread&lt;/a&gt; on SEQanswers discusses a variety of BAM-to-Fastq converters that exist or have existed.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt;I haven't discussed pairing information since it's not important for the current analysis I'm doing, but it should also be simple to extract by examining the read ID and bitwise flag(s) related to pairing.
Of course, things are rarely as simple in practice as they are conceptually...&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt;This is actually a very important discussion, and where the publishing-BAM-only approach becomes tricky.
When do we handle adapter trimming? Removal of technical duplicates? Quality trimming (if any)?
Wouldn't we want to apply some quality control before mapping the reads?
But then again, quality control on reads is not a settled science, and publishing BAM files of cleaned-up reads prevents us from applying improved QC in the future.
I'm not sure I have a good answer for this, but it's an important conversation.&lt;/p&gt;</content><category term="mapping"></category><category term="bam"></category></entry><entry><title>Reproducible variant calling is possible with randomized algorithms</title><link href="https://standage.github.io/reproducible-variant-calling-is-possible-with-randomized-algorithms.html" rel="alternate"></link><published>2016-08-22T00:00:00-04:00</published><updated>2016-08-22T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-08-22:/reproducible-variant-calling-is-possible-with-randomized-algorithms.html</id><summary type="html">&lt;p&gt;This morning I read &lt;a href="http://dx.doi.org/10.1093/bioinformatics/btw139"&gt;On genomic repeats and reproducibility&lt;/a&gt; by Can Firtina and Can Alkan.
The paper discusses two notable observations regarding calling genomic variants.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Some sequence read aligners are not deterministic, and shuffling the order of the reads can result in different alignments.&lt;/li&gt;
&lt;li&gt;Some variant callers are not deterministic, and will report a different set of variants if an â€¦&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;This morning I read &lt;a href="http://dx.doi.org/10.1093/bioinformatics/btw139"&gt;On genomic repeats and reproducibility&lt;/a&gt; by Can Firtina and Can Alkan.
The paper discusses two notable observations regarding calling genomic variants.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Some sequence read aligners are not deterministic, and shuffling the order of the reads can result in different alignments.&lt;/li&gt;
&lt;li&gt;Some variant callers are not deterministic, and will report a different set of variants if an analysis is repeated on the same set of alignments (i.e. the same BAM file).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Non-deterministic read mapping&lt;/h2&gt;
&lt;p&gt;The first observation wasn't all that surprising to me.
I remember as a graduate student hearing anecdotally that genome assembly algorithms are (or can be) sensitive to read order.
And anything that involves pseudorandom number generators will by definition produce results with technical variation.&lt;/p&gt;
&lt;p&gt;The authors analyzed four read alignment tools: Bowtie2, RazerS3, mrFAST, and BWA-MEM.
Based on a sample of 1 million reads, they confirmed that Bowtie2&lt;sup&gt;â€¡&lt;/sup&gt; and mrFAST produce identical alignments when the read order is shuffled.
BWA-MEM, however, reports multimapped reads differently, and the location reported for each multimapped read depends on its order.
Oddly enough, the the authors didn't report RazerS3 results for this analysis, although in the discussion section they describe RazerS3 as a deterministic mapper.&lt;/p&gt;
&lt;h2&gt;Non-deterministic variant calling&lt;/h2&gt;
&lt;p&gt;The second observation was quite intriguing to me.
The authors evaluated four variant callersâ€”HaplotypeCaller, Freebayes, Platypus, and SAMtoolsâ€”and reported the extent to which differences in read alignments due to read order affect variant calling.
More interestingly, though, was the observation that HaplotypeCaller would produce different results when run multiple times on the same input file.
Apparently, HaplotypeCaller uses a random sampling of the training data for sake of efficiency, and different random samples will result in different variants being filtered out by quality control before final results are reported.&lt;/p&gt;
&lt;h2&gt;Discussion and recommendations&lt;/h2&gt;
&lt;p&gt;In the discussion section, the authors recommend using a deterministic read mapper and variant caller for sake of reproducibility.
However, I want to contend with their claim that &lt;em&gt;Full reproducibility could only be achieved through using deterministic methods.&lt;/em&gt;
The behavior of random number generators can be controlled by initializing the generator with a "seed" (see &lt;a href="https://biowize.wordpress.com/2015/08/05/reproducible-software-behavior-with-random-seeds/"&gt;this post from my old blog&lt;/a&gt;), and I think I'm safe in saying that random seeds are general knowledge.
Somebody with enough technical chops to implement a read aligner will almost certainly be familiar with random seeds.
Therefore, a randomized algorithm can indeed be fully reproducible if the following conditions are met.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The algorithm allows (but does not necessarily require) the user to specify a seed to initialize the random number generator.&lt;/li&gt;
&lt;li&gt;The algorithm reports the seed used to initialize the random number generator, whether specified by the user or not.&lt;/li&gt;
&lt;li&gt;The random seed(s) used for an analysis are disclosed whenever results are published or shared.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, BWA-MEM does not allow users to specify random seeds, so the authors are correct &lt;em&gt;in this case&lt;/em&gt; that full reproducibility is not possible with the aligner (I'm not familiar with HaplotypeCaller, so I won't comment on that).
However, the authors themselves admit that in general "randomized algorithms may achieve better accuracy in practice."
So rather than recommending that fellow scientists avoid a tool or class of tools entirely for the sake of reproducibility, I contend that we should recommend that randomized algorithms follow these simple steps to facilitate complete reproducibility.
Perhaps I'll even open a pull request for BWA-MEM myself, although with 30 open pull requests I don't have faith it would be merged any time soon.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;sup&gt;â€¡&lt;/sup&gt;The authors reported anecdotal evidence, almost as an afterthought, that changing read names will affect the results reported by Bowtie2, even if changing the read order does not.&lt;/p&gt;</content><category term="random"></category><category term="variant calling"></category></entry><entry><title>That darn cache! Configuring the SRA Toolkit</title><link href="https://standage.github.io/that-darn-cache-configuring-the-sra-toolkit.html" rel="alternate"></link><published>2016-05-18T00:00:00-04:00</published><updated>2016-05-18T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-05-18:/that-darn-cache-configuring-the-sra-toolkit.html</id><summary type="html">&lt;p&gt;Last night I started a batch job on our group's cluster to download and process 9 Illumina libraries from the &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;NCBI SRA&lt;/a&gt;.
In the past, I have almost always downloaded such data via direct links to &lt;code&gt;.sra&lt;/code&gt; files on the SRA FTP site, and then converted these files to Fastq format using the &lt;code&gt;fastq-dump&lt;/code&gt; command from the &lt;a href="https://www.ncbi.nlm.nih.gov/Traces/sra/?view=software"&gt;SRA Toolkit&lt;/a&gt;.
However â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last night I started a batch job on our group's cluster to download and process 9 Illumina libraries from the &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;NCBI SRA&lt;/a&gt;.
In the past, I have almost always downloaded such data via direct links to &lt;code&gt;.sra&lt;/code&gt; files on the SRA FTP site, and then converted these files to Fastq format using the &lt;code&gt;fastq-dump&lt;/code&gt; command from the &lt;a href="https://www.ncbi.nlm.nih.gov/Traces/sra/?view=software"&gt;SRA Toolkit&lt;/a&gt;.
However, for a while I've been aware that the &lt;code&gt;fastq-dump&lt;/code&gt; command is capable of downloading data (by accession) directly from NCBI.
So last night, I took advantage of this convenience function.&lt;/p&gt;
&lt;p&gt;This morning, I woke up to a barrage of warnings on the cluster saying that their was not disk space left.
&lt;em&gt;Impossible&lt;/em&gt;, I thought, &lt;em&gt;there are terabytes of free space on the scratch mount.&lt;/em&gt;
It was not the scratch partition that had filled up though, it was the &lt;code&gt;$HOME&lt;/code&gt; partition.
I'm lucky I was even able to log on to the machine.
What the heck is going on?!?!&lt;/p&gt;
&lt;p&gt;It turns out that by default, &lt;code&gt;fastq-dump&lt;/code&gt; doesn't actually stream &lt;code&gt;.sra&lt;/code&gt; files from the FTP site.
Instead, it downloads intermediate cache files, which by default are stored in &lt;code&gt;~/ncbi/sra/&lt;/code&gt;.
This is a huge issue for several reasons.
First of all, this behavior is not &lt;em&gt;at all&lt;/em&gt; obvious to a new user.
Sure, there may be use cases where this behavior is very useful, but it seems much more appropriate as an &lt;em&gt;opt-in&lt;/em&gt; feature than a default.
Secondly, this is bound to create problems for cluster/HPC users all over the place that have very limited storage in their &lt;code&gt;$HOME&lt;/code&gt; directories.&lt;/p&gt;
&lt;p&gt;Fortunately, the SRA Toolkit allows you to configure this behavior.
The &lt;a href="https://github.com/ncbi/sra-tools/wiki/Toolkit-Configuration"&gt;officially-sanctioned approach&lt;/a&gt; is to fire up a config program, which will allow you to disable caching behavior and/or remote network access of any kind.
The unofficial unsanctioned approach would be to make the config file yourself with a couple of shell commands.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir -p ~/.ncbi
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/repository/user/main/public/root = &amp;quot;/scratch/standage/sra-cache&amp;quot;&amp;#39;&lt;/span&gt; &amp;gt; ~/.ncbi/user-settings.mkfg
&lt;span class="c1"&gt;# Uncomment the next command if you want to disable network access altogether&lt;/span&gt;
&lt;span class="c1"&gt;# echo &amp;#39;/repository/user/cache-disabled = &amp;quot;true&amp;quot;&amp;#39; &amp;gt; ~/.ncbi/user-settings.mkfg&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="sra"></category><category term="ngs"></category></entry><entry><title>The eduroam network and 802.1X profiles in Mac OS X</title><link href="https://standage.github.io/the-eduroam-network-and-8021x-profiles-in-mac-os-x.html" rel="alternate"></link><published>2016-05-18T00:00:00-04:00</published><updated>2016-05-18T00:00:00-04:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-05-18:/the-eduroam-network-and-8021x-profiles-in-mac-os-x.html</id><summary type="html">&lt;p&gt;My affiliation recently changed from Indiana University to UC Davis, and accordingly my IU credentials no longer give me access to the eduroam wifi network.
Over the last couple of days I've been struggling to connect my laptop to eduroam using my new UC Davis credentials.
At first I thought the it was an issue with my account, but it â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;My affiliation recently changed from Indiana University to UC Davis, and accordingly my IU credentials no longer give me access to the eduroam wifi network.
Over the last couple of days I've been struggling to connect my laptop to eduroam using my new UC Davis credentials.
At first I thought the it was an issue with my account, but it turns out it was simply an issue with a profile saved on the system.&lt;/p&gt;
&lt;p&gt;When I first connected to eduroam with my IU credentials on my MacBook Pro several years ago, it stored a "profile" on the system.
On my phone, "forgetting" the eduroam network was sufficient for deleting this profile, but on the laptop, disconnecting or "forgetting" the network didn't solve the issue.
With a bit of searching, I was able to find the profile in the &lt;code&gt;Network&lt;/code&gt; section of &lt;code&gt;System Preferences&lt;/code&gt;, but this provided no way to remove the profile (as had been suggested on several forums for older versions of OS X)!&lt;/p&gt;
&lt;p&gt;Finally, I was able to find the profile in a different subsection of the &lt;code&gt;System Preferences&lt;/code&gt; (aptly named &lt;code&gt;Profiles&lt;/code&gt;, go figure), and there was able to delete the profile.
I was then able to connect to the eduroam network, provide my UC Davis credentials, and successfully connect.
It doesn't look like UC Davis stored a profile on the system, so hopefully I don't have to deal with this issue again!&lt;/p&gt;</content><category term="networking"></category><category term="mac"></category></entry><entry><title>Citing "manuscripts in progress" on your CV</title><link href="https://standage.github.io/citing-manuscripts-in-progress-on-your-cv.html" rel="alternate"></link><published>2016-02-26T10:00:00-05:00</published><updated>2016-02-26T10:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-02-26:/citing-manuscripts-in-progress-on-your-cv.html</id><summary type="html">&lt;p&gt;A couple of weeks ago, I saw a couple of Twitter threads explode on the topic of citing "manuscripts in progress" on one's CV.&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;OK I just saw an application that listed a prestigious journal name for a paper that is IN PREPARATION. Not even submitted. &lt;a href="https://twitter.com/hashtag/stopthemadness?src=hash"&gt;#stopthemadness&lt;/a&gt;&lt;/p&gt;&amp;mdash; Michael Hoffman (@michaelhoffman) &lt;a href="https://twitter.com/michaelhoffman/status/696758391239221248"&gt;February 8, 2016&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Postdoc applicants - seriously, don&amp;#39;t do this â€¦&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;A couple of weeks ago, I saw a couple of Twitter threads explode on the topic of citing "manuscripts in progress" on one's CV.&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;OK I just saw an application that listed a prestigious journal name for a paper that is IN PREPARATION. Not even submitted. &lt;a href="https://twitter.com/hashtag/stopthemadness?src=hash"&gt;#stopthemadness&lt;/a&gt;&lt;/p&gt;&amp;mdash; Michael Hoffman (@michaelhoffman) &lt;a href="https://twitter.com/michaelhoffman/status/696758391239221248"&gt;February 8, 2016&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Postdoc applicants - seriously, don&amp;#39;t do this: &lt;a href="https://t.co/o0d3x5rSTT"&gt;https://t.co/o0d3x5rSTT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Daniel MacArthur (@dgmacarthur) &lt;a href="https://twitter.com/dgmacarthur/status/696892638721474560"&gt;February 9, 2016&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;I include a retweet along with the original tweet, since the discussion on both threads is informative.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There seems to be a pretty wide consensus among academics (at least those on Twitter) that listing publications &lt;em&gt;in progress&lt;/em&gt; with a journal name is pretty pretentious and is likely to do more harm than good.
Of course all of this depends on context.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An undergrad applying for a Master's or Ph.D. program would be treated differently than a Ph.D. student applying for a postdoc, or a postdoc applying for a faculty position.&lt;/li&gt;
&lt;li&gt;If the CV has several solid publications and only one or two listed as &lt;em&gt;in progress&lt;/em&gt;, it would be treated differently than a CV with 1 (or 0) actual publications and several &lt;em&gt;in progress&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So should you list &lt;em&gt;in progress&lt;/em&gt; manuscripts/projects on your CV?
Despite all the pitchfork brandishing that accompanied these Twitter threads, I'm actually of the opinion that this is an honest and acceptable way to present your scholarship, with a few caveats.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Don't list the journal name!&lt;/strong&gt;
  I completely agree that listing a specific journal along with an &lt;em&gt;in prep&lt;/em&gt; paper is ridiculous and pretentious.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clearly list the manuscript as &lt;em&gt;in preparation&lt;/em&gt;!&lt;/strong&gt;
  Not everyone looks kindly on listing &lt;em&gt;in prep&lt;/em&gt; manuscripts on your CV, and you don't want anyone to think you are being dishonest about your scholarship.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Only list papers that are soon to be submitted!&lt;/strong&gt;
  If your paper is a long way from submission, listing it as &lt;em&gt;in prep&lt;/em&gt; is little more than wishful thinking.
  This isn't really being honest with yourself or with others, and it would be ridiculous to have to &lt;em&gt;remove&lt;/em&gt; an item from your CV later on because life intervened and you were never able to complete a project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consider your academic status!&lt;/strong&gt;
  As you advance in your career, the need to list &lt;em&gt;in prep&lt;/em&gt; manuscripts on your CV should rapidly dissolve.
  People will be understanding if you're applying for grad school, but you have to be much more careful when finishing grad school and applying for postdocs.
  By the time you're applying for faculty positions your publication record should &lt;em&gt;really&lt;/em&gt; be strong enough to stand on its own without padding from &lt;em&gt;in prep&lt;/em&gt; manuscripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My suggestion is that early career scientists may want to replace the &lt;strong&gt;Publications&lt;/strong&gt; section of their CV with a more general &lt;strong&gt;Scholarship&lt;/strong&gt; section, with subsections for &lt;strong&gt;Publications&lt;/strong&gt;, &lt;strong&gt;Posters&lt;/strong&gt;, and &lt;strong&gt;Oral Presentations&lt;/strong&gt;.
The &lt;strong&gt;Publications&lt;/strong&gt; subsection would be strictly for actual published papers and accepted in-press papers (and &lt;em&gt;maybe&lt;/em&gt; submitted papers, with discretion).
For substantial projects that have not yet been published, hopefully you've had a chance to present that work to your academic colleagues at a research conference.
In that case, list poster and oral presentations, along with the conference name and details, in the relevant subsection(s).
The end of one of these entries, in my opinion, would be the place to list an unfinished paper as &lt;em&gt;in preparation&lt;/em&gt;, subject to the same caveats listed above.&lt;/p&gt;
&lt;p&gt;What to you think?&lt;/p&gt;</content><category term="science"></category><category term="publication"></category></entry><entry><title>Searching for TSA master records at NCBI</title><link href="https://standage.github.io/searching-for-tsa-master-records-at-ncbi.html" rel="alternate"></link><published>2016-02-19T00:00:00-05:00</published><updated>2016-02-19T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-02-19:/searching-for-tsa-master-records-at-ncbi.html</id><summary type="html">&lt;p&gt;The &lt;a href="http://www.ncbi.nlm.nih.gov/genbank/tsa"&gt;NCBI Transcript Shotgun Assembly database&lt;/a&gt; is the go-to place for submitting transcript assemblies for long-term archival and public access.
However, &lt;code&gt;TSA&lt;/code&gt; is not one of the database options provided when doing keyword searches at NCBI.
TSA sequences are available through the &lt;code&gt;nuccore&lt;/code&gt; nucleotide database, along with all other DNA and RNA sequences.&lt;/p&gt;
&lt;p&gt;If you want to search NCBI exclusively â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="http://www.ncbi.nlm.nih.gov/genbank/tsa"&gt;NCBI Transcript Shotgun Assembly database&lt;/a&gt; is the go-to place for submitting transcript assemblies for long-term archival and public access.
However, &lt;code&gt;TSA&lt;/code&gt; is not one of the database options provided when doing keyword searches at NCBI.
TSA sequences are available through the &lt;code&gt;nuccore&lt;/code&gt; nucleotide database, along with all other DNA and RNA sequences.&lt;/p&gt;
&lt;p&gt;If you want to search NCBI exclusively for TSA records, include &lt;code&gt;tsa-master[prop]&lt;/code&gt; in your query.
For example, if you want to look for TSA records for the wasp &lt;em&gt;Polistes metricus&lt;/em&gt;, select the &lt;code&gt;Nucleotide&lt;/code&gt; database and use the query &lt;code&gt;tsa-master[prop] AND "Polistes metricus"[Organism]&lt;/code&gt;.
Similarly, the query &lt;code&gt;tsa-master[prop] AND Polistes[Organism]&lt;/code&gt; will search the entire genus.
The final entry of each TSA master record is a &lt;code&gt;TSA&lt;/code&gt; attribute, with a link to download the sequences in compressed Fasta format.&lt;/p&gt;</content><category term="ncbi"></category><category term="sequences"></category></entry><entry><title>On genomic interval notation</title><link href="https://standage.github.io/on-genomic-interval-notation.html" rel="alternate"></link><published>2016-02-11T00:00:00-05:00</published><updated>2016-02-11T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-02-11:/on-genomic-interval-notation.html</id><summary type="html">&lt;p&gt;Intervals are one of the most common data abstractions used in genome informatics, along with strings and graphs.
DNA has an intricate dynamic three-dimensional structure, but for many bioinformatics applications we can get away with ignoring this level of detail and representing the molecule instead as a static linear sequence of symbols.
Genomic featuresâ€”such as genes or transposable elements â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Intervals are one of the most common data abstractions used in genome informatics, along with strings and graphs.
DNA has an intricate dynamic three-dimensional structure, but for many bioinformatics applications we can get away with ignoring this level of detail and representing the molecule instead as a static linear sequence of symbols.
Genomic featuresâ€”such as genes or transposable elementsâ€”are then annotated as subsequences of the larger complete sequence, much like an interval on a discrete number line.&lt;/p&gt;
&lt;p&gt;The most popular data formats for encoding sequence annotations (&lt;a href="http://www.sequenceontology.org/gff3.shtml"&gt;GFF3&lt;/a&gt;, &lt;a href="https://genome.ucsc.edu/FAQ/FAQformat.html#format1"&gt;BED&lt;/a&gt;, &lt;a href="http://mblab.wustl.edu/GTF22.html"&gt;GTF&lt;/a&gt;) all use a very similar convention.
The location of a feature is designated by three values: some kind of &lt;em&gt;label&lt;/em&gt; or &lt;em&gt;identifier&lt;/em&gt; specifying the precise molecule on which the feature resides (such as &lt;em&gt;Chr1&lt;/em&gt; or &lt;em&gt;Scaff17468&lt;/em&gt;), and two integers representing the start and end positions of the feature on that molecule.
These values are stored in tab-delimited plain text, along with various other values and metadata.&lt;/p&gt;
&lt;p&gt;There are, however, some substantial differences between "the big three" formats.
BED and GTF were designed for very specific use cases (visualization and gene prediction, respectively), whereas GFF3 was designed as a generalized solution for genome annotation.
BED allows for a single level of feature decomposition (a feature can be broken up into blocks) and GTF supports two levels (exons can be grouped by &lt;code&gt;transcript_id&lt;/code&gt;, and transcripts grouped by &lt;code&gt;gene_id&lt;/code&gt;), while GFF3 supports an arbitrary number of levels (parent/child relationships defined by &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;Parent&lt;/code&gt; attributes specify a directed acyclic graph of features).&lt;/p&gt;
&lt;p&gt;Perhaps the most important difference is the &lt;em&gt;notation&lt;/em&gt; these formats use to encode genomic intervals: that is, which two integers are used to specify the location of the interval?
GFF3 and GTF both inherited, through their common heritage with older GFF variants, 1-based indexing and closed interval notation.
BED on the other hand uses 0-based indexing and &lt;em&gt;half-closed&lt;/em&gt; interval notation.
Consider the DNA sequence below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        1    2    3    4    5    6    7      &amp;lt;-- GFF3 style
        |    |    |    |    |    |    |
        G    A    T    T    A    C    A
        |    |    |    |    |    |    |
        0    1    2    3    4    5    6      &amp;lt;-- BED style
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using 1-based indexing and closed interval notation &lt;em&gt;a la&lt;/em&gt; GFF3, the interval containing the subsequence &lt;code&gt;ATTA&lt;/code&gt; would be represented as &lt;code&gt;[2, 5]&lt;/code&gt;.
With BED's 0-based indexing and half-closed interval notation, the same subsequence would be represented as &lt;code&gt;[1, 5)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Confused yet?
Don't worry: off-by-one errors with interval arithmetic are very common in bioinformatics, striking newcomers and old-timers alike.&lt;/p&gt;
&lt;p&gt;An alternative and useful way to conceptualize the BED-style notation is to shift the indices slightly so that they correspond to the spaces &lt;em&gt;between&lt;/em&gt; nucleotides, rather than the nucleotides themselves.
Then, the two integers defining an interval are the ones that bound the nucleotides in question.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        G    A    T    T    A    C    A
     |    |    |    |    |    |    |    |
     0    1    2    3    4    5    6    7
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, the big question is: are there any compelling reasons to choose one convention over the other?
Or is it simply a matter of preference?&lt;/p&gt;
&lt;p&gt;Until very recently, my opinion has been that it's primarily a matter of preference.
For a variety of other reasons, I have long favored the GFF3 format over any of the alternatives.
Leveraging the &lt;a href="http://www.sequenceontology.org"&gt;Sequence Ontology&lt;/a&gt; and supporting parent/child relationships provides a more flexible, comprehensive, and consistent solution for genome annotation than any of GFF3's tab-delimited relatives.
And counting nucleotides starting from 1 never really felt unnatural, at least from the perspective of a biologist.&lt;/p&gt;
&lt;p&gt;But I'm now convinced that BED-style interval notation is indisputably superior to GFF-style interval notation.
Consider the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One of the most widely cited benefits of BED-style notation is that interval length calculations are much simpler: &lt;code&gt;end - start&lt;/code&gt; rather than &lt;code&gt;end - start + 1&lt;/code&gt;.
  Interval overlap calculations enjoy the same benefit: &lt;code&gt;min(a.end, b.end) - max(a.start, b.start)&lt;/code&gt; instead of &lt;code&gt;min(a.end, b.end) - max(a.start, b.start) + 1&lt;/code&gt;.
  In terms of &lt;em&gt;performance&lt;/em&gt;, I doubt removing these extra operations will make much of a difference for the vast majority of bioinformatics software.
  There is a lot to be gained by making &lt;em&gt;code&lt;/em&gt; cleaner, simpler, and easier to read, though.
  It's important to optimize code for human comprehension whenever possible, and removing superfluous &lt;code&gt;+1&lt;/code&gt;s and &lt;code&gt;-1&lt;/code&gt;s can go a long way in this regard.&lt;/li&gt;
&lt;li&gt;When taking the reverse complement of a sequence, the interval &lt;code&gt;[start, end)&lt;/code&gt; becomes &lt;code&gt;[length - end, length - start)&lt;/code&gt;, rather than &lt;code&gt;[length - end + 1, length - start + 1)&lt;/code&gt;.
  And honestly, the list of cases where the choice of notation makes these pesky &lt;code&gt;+1&lt;/code&gt;s and &lt;code&gt;-1&lt;/code&gt;s disappear keeps going and going, so I'll just leave it at that.&lt;/li&gt;
&lt;li&gt;When splitting a sequence into, for example, 100kb chunks, the BED-style notation gives much cleaner-looking boundaries: &lt;code&gt;[100000, 200000), [200000, 300000)&lt;/code&gt; instead of &lt;code&gt;[100001, 200000], [200001, 300000]&lt;/code&gt; or &lt;code&gt;[100000, 199999], [200000, 299999]&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Another compelling argument is that most programming languages utilize 0-based indexing, so using GFF-style notation requires care on the part of the programmer to make the ajustments herself when performing interval/range operations, such as accessing a DNA subsequence stored as a string.
  The BED-style notation leads to very clean C-style loops (&lt;code&gt;for(int i = start; i &amp;lt; end; i++)&lt;/code&gt;), and ranges and slices in Python use the same convention.
  Back in the 80s, before the computer science community had settled on 0-based indexing, renowned computer scientist &lt;a href="http://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html"&gt;Edsger Dijkstra's argument for 0-based indexing&lt;/a&gt; included a simple yet elegant defense of 0-based half-closed interval notation, citing empirical evidence that its use leads to fewer programming errors than any alternative notation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As persuasive as this list is, one might claim that all of these points (perhaps with the exception of Dijkstra's data-supported claims) still fall within the realm of opinion.
But where the GFF-style notation really falls apart is in the encoding of &lt;strong&gt;zero-length features&lt;/strong&gt;.
GFF3 simply has no unambiguous notation for specifying insertion sites, cleavage sites, or any other feature that resides &lt;em&gt;between&lt;/em&gt; nucleotides.
To be clear, the GFF3 specification explicitly addresses this case.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For zero-length features, such as insertion sites, start equals end and the implied site is to the right of the indicated base in the direction of the landmark.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But how is one to distinguish zero-length features from one-length features, which must also be encoded as &lt;code&gt;start = end&lt;/code&gt;?
There is no way to differentiate these two cases without additional (and probably non-canonical) contextual hints, which, as experience shows, come in about as many flavors as there are bioinformaticians.&lt;/p&gt;
&lt;p&gt;So although I still personally prefer GFF3 to the competitors, I now acknowledge the use of 1-based closed intervals as one of its biggest weaknesses.
That leaves me with a couple of options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I could bite the bullet and start using the BED format, despite its shortcomings.&lt;/li&gt;
&lt;li&gt;I could push to change GFF3 to use a demonstrably superior interval notation.&lt;/li&gt;
&lt;li&gt;I could continue using GFF3, despite its shortcomings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My hesitation with the first option is that BED is so loosely defined that it barely passes for a "standard" format.
Of course, people adopt different conventions with all of these formats, some of which are clearly incorrect, and many of which are valid but simply different alternative representations of the same data.
But BED gives &lt;em&gt;so much leeway&lt;/em&gt; (user's choice of whitespace or tabs for separating fields, optional fields, and sanctioned derivative formats such as "BED detail") that rigorous checks of data integrity can become quite tedious.
Not that the majority of bioinformatics programmers actually &lt;em&gt;do&lt;/em&gt; rigorous error checking, but you know...&lt;/p&gt;
&lt;p&gt;As much as I would welcome an update to the GFF3 spec, I'm afraid interval notation is too central a feature to change with an incremental update.
It would likely require a new major release of the specification, and with so much code depending on GFF3, I'm not sure the world is quite ready for GFF4.&lt;/p&gt;
&lt;p&gt;So I think for now, my best option is to just stick with GFF3, although I might consider using BED-style notation to represent intervals internally in my software.
If I understand correctly, many of the GMOD tools (which are responsible in large part for the success of GFF3) do this, so it's not an unreasonable approach.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hat tip to &lt;a href="https://github.com/ga4gh/schemas/pull/49#issuecomment-44520397"&gt;this thread&lt;/a&gt; for the insightful discussion and link to the Dijkstra transcript, and to &lt;a href="https://www.biostars.org/p/176583/#176590"&gt;this post&lt;/a&gt; for suggesting that zero-based indices point &lt;em&gt;between&lt;/em&gt; nucleotides.&lt;/p&gt;</content><category term="annotation"></category><category term="gff3"></category><category term="bed"></category><category term="intervals"></category></entry><entry><title>My tutorial on git banches</title><link href="https://standage.github.io/my-tutorial-on-git-banches.html" rel="alternate"></link><published>2016-02-05T00:00:00-05:00</published><updated>2016-02-05T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-02-05:/my-tutorial-on-git-banches.html</id><summary type="html">&lt;p&gt;I've been trying subtly (and in a few cases not-so-subtly) for years now to convert my colleagues to the gospel of git and GitHub.
The git version control system has its quirks no doubt, but there isâ€”in my opinionâ€”no more powerful system for open collaboration on software and science than git and GitHub.
A quote I attribute (hopefully â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been trying subtly (and in a few cases not-so-subtly) for years now to convert my colleagues to the gospel of git and GitHub.
The git version control system has its quirks no doubt, but there isâ€”in my opinionâ€”no more powerful system for open collaboration on software and science than git and GitHub.
A quote I attribute (hopefully correctly) to Software Carpentry's Greg Wilson (&lt;a href="https://twitter.com/gvwilson"&gt;@gvwilson&lt;/a&gt;) captures my sentiments exactly.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning git is the tax you pay to use GitHub.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Over Christmas break my advisor finally decided it was time to bite the bullet and see what git/GitHub offered that he couldn't get with Subversion.
It took him a while to warm up to the idea, but once he decided to try it, he was committed to giving it his best shot.
He read up on the git/GitHub workflow and immersed himself in it for a week or more.
By the time we reconnected in the new year, he was convinced this was something he wanted to adopt for himself and the rest of the lab.&lt;/p&gt;
&lt;p&gt;Since then, we've been gradually piecing together a lab handbook with lab procedures, guidelines, and tips.
My advisor wrote up a basic git/GitHub tutorial a few weeks ago, and yesterday I wrote a follow-up on using git branches in your daily workflow.
The text of the tutorial is below.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;01.1-Howto-github-branches&lt;/h1&gt;
&lt;p&gt;The git version control system has a feature called &lt;em&gt;branches&lt;/em&gt; that make it safe and easy to implement bug fixes or new features, and to isolate those changes from the main development environment until they are vetted.
The concept of branches is not unique to git: users of Subversion or CVS will recognize the common organizational strategy of placing the main development environment in a &lt;code&gt;trunk/&lt;/code&gt; directory, and then creating experimental branches by copying that directory to a separate &lt;code&gt;branches/&lt;/code&gt; directory.
Indeed, with Subversion, branching is just thatâ€”a strategyâ€”and merging branches back into the main trunk can be very tedious and frustrating.
With git, however, branches are a built-in feature: they are trivial to create, and can almost always be merged back into the main line of development (&lt;em&gt;master&lt;/em&gt;) with little or no hassle.&lt;/p&gt;
&lt;p&gt;For a quick intro to git branches, see &lt;a href="https://www.atlassian.com/git/tutorials/using-branches/git-branch"&gt;this tutorial from Atlassian&lt;/a&gt;.
The remainder of this HowTo presents a hypothetical vignette demonstrating how branches could and should be used as part of your daily workflow, and how they help mitigate merge conflicts when working collaboratively.&lt;/p&gt;
&lt;h2&gt;Setting&lt;/h2&gt;
&lt;p&gt;Alice and Bob are two scientists collaborating on a research project they hope to publish soon.
They have implemented a prototype R script to analyze data they have collected, and the R code is stored in a repository on GitHub called &lt;code&gt;BrendelGroup/proj&lt;/code&gt;.
Alice and Bob have their own forks at &lt;code&gt;alice/proj&lt;/code&gt; and &lt;code&gt;bob/proj&lt;/code&gt;, respectively.
Having prototyped the analysis procedure and tested it on small sample data sets, they are ready to move forward by polishing up the prototype, running the analysis on complete data sets, reporting their results, and discussing their interpretation.&lt;/p&gt;
&lt;p&gt;Monday morning, Alice and Bob spent some time working on their manuscript, documenting the motivation for their work, describing their methodology, and putting in placeholders for results they will soon generate.
Monday afternoon, after working on the manuscript, they discussed what  needed to be done next at a technical level.
- First, they need to fix a small bug in the R prototype.
  They opened a new thread on the &lt;code&gt;BrendelGroup/proj&lt;/code&gt; issue tracker on GitHub to describe the bug, what causes the script to fail, and their plan to fix the bug.
  The issue tracker designated this thread &lt;code&gt;#14&lt;/code&gt;.
- Second, they need to add one more step to their analysis procedure.
  They described this step at (a high level) in the &lt;em&gt;Methods&lt;/em&gt; section of their paper earlier that morning, but they took some time now to start a new thread on the GitHub issue tracker and describe their precise plan for implementating this additional step in R.
  The issue tracker designated this thread &lt;code&gt;#15&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;First branch&lt;/h2&gt;
&lt;p&gt;On Tuesday morning, Alice comes in to the office to work on the project.
She decides to get to work right away fixing the bug they discussed the previous day.
She logs in to her laptop, fires up a terminal, changes to the directory containing her local clone of the repository, and checks her status...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd /home/alice/proj/
git status
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...to which her terminal responds as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;On branch master
nothing to commit, working directory clean
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;She's on the main/default development branch &lt;em&gt;master&lt;/em&gt;, and the working directory is &lt;em&gt;clean&lt;/em&gt;, meaning no changes have been made to the code since the last commit.&lt;/p&gt;
&lt;p&gt;Since everything looks good, Alice creates a new branch called &lt;code&gt;bugfix&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git branch bugfix
git checkout bugfix
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;She then opens up the R script and takes about 30 minutes to fix the code and test her solution.
Once she's confident it is correct, she commits a new snapshot to her local repository.
She references the corresponding GitHub thread in her commit message, using syntax that will automatically close the thread in the issue tracker once the commit is merged into the &lt;code&gt;master&lt;/code&gt; branch of &lt;code&gt;BrendelGroup/proj&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git add analysis.R
git commit -m &amp;#39;Fixed estimation bug. Closes #14.&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Pull request&lt;/h2&gt;
&lt;p&gt;At this point, the git repository on Alice's laptop has the bugfix, but the changes haven't been pushed anywhere else: her fork on GitHub (&lt;code&gt;alice/proj&lt;/code&gt;), the original "upstream" repository on GitHub (&lt;code&gt;BrendelGroup/proj&lt;/code&gt;), or Bob's fork (&lt;code&gt;bob/proj&lt;/code&gt;).
Referring back to the creative triangle described in &lt;a href="https://github.com/BrendelGroup/bghandbook/blob/master/doc/01-Howto-github.md"&gt;the GitHub howto&lt;/a&gt;, Alice proceeds by pushing her latest commit (stored on a branch called &lt;code&gt;bugfix&lt;/code&gt;) to her fork on GitHub.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin bugfix
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;She now goes to GitHub and opens a pull request to merge &lt;code&gt;alice/proj:bugfix&lt;/code&gt; into &lt;code&gt;BrendelGroup/proj:master&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Second branch&lt;/h2&gt;
&lt;p&gt;With the bug fix resolved, Alice now turns to the final step in their analysis procedure.
The bug has nothing to do with this final step, so she starts by returning to the master branch.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Her directory now looks just like it did before she started working this morning.
This provides a clean slate for Alice to begin working on the analysis procedure.
She proceeds by creating a new branch in her local repo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git branch laststep
git checkout laststep
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;She then takes an hour or so to prepare a few small data files she will use to test the R script after the new feature is implemented.
Next, once she is confident the tests are correct, she opens up the R script again and implements the new procedure.
With test data already in place, it is very easy for Alice to confirm when the procedure is working correctly.&lt;/p&gt;
&lt;p&gt;Finally, after all her tests pass, Alice commits a new snapshot in her local repository.
Again, she references the corresponding thread on the GitHub issue tracker.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git add analysis.R testdatafiles/
git commit -m &amp;#39;Implemented final analysis step. Closes #15.&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Another pull request&lt;/h2&gt;
&lt;p&gt;Just as before, Alice's new feature is present only on her laptop.
She needs to push out to her fork and open a pull request to get these changes integrated into the main repository.&lt;/p&gt;
&lt;p&gt;First she pushes to her fork...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin laststep
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and then goes to GitHub to open a pull request to merge &lt;code&gt;alice/proj:laststep&lt;/code&gt; into &lt;code&gt;BrendelGroup/proj:master&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At this point, she has two open pull requests waiting for Bob to review and accept.
Having worked for a few hours at this point, Alice takes a break to have lunch and attend a seminar.&lt;/p&gt;
&lt;h2&gt;Bob's review&lt;/h2&gt;
&lt;p&gt;When Bob arrives at his office late on Tuesday morning, he's suspicious that the solution he discussed with Alice the day before is incorrect.
After digging through the data for a bit, he confirms his suspicion that they have made some incorrect assumptions about the input data, and that the bugfix they discussed previously isn't valid.
He logs on to GitHub to comment on thread &lt;code&gt;#14&lt;/code&gt; and explain his findings.
Bob notes that Alice has already submitted a pull request to fix the bug, and adds a comment to the thread explaining why their proposed solution won't work.&lt;/p&gt;
&lt;p&gt;Bob also notes that Alice has submitted a pull request with her implementation of the final step of the procedure.
He reviews the changes to the code on her &lt;code&gt;laststep&lt;/code&gt; branch, understands the changes she made, and notes that she has provided test cases for validating the procedure.
Since everything looks good, Bob accepts the pull request.
GitHub automatically closes issue &lt;code&gt;#15&lt;/code&gt; at this point, since Alice's commit message included the text &lt;code&gt;Closes #15&lt;/code&gt;.
Bob opens up his terminal, goes to his local clone of the repository, and pulls the latest changes from &lt;code&gt;BrendelGroup/proj&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd /home/bob/proj
git pull upstream master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;He then pushes those changes out to his fork to keep everything in sync.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin master
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Bob's bug fix&lt;/h2&gt;
&lt;p&gt;After his morning data slogging session, Bob has a much better grasp on how to fix the bug in their R code.
He has a few minutes before he has to teach a class, so he takes a stab at implementing the bug fix.
Bob starts by creating a new branch in his local repo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git branch newbugfix
git checkout newbugfix
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;He then edits the R code and tests that it works on a variety of inputs.
Once he's confident the bug fix handles the input data correctly, he makes a new commit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git add analysis.R
git commit -m &amp;#39;Alternative approach to fixing the estimation bug. See my comments in #14. Closes #14.&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bob then pushes this new branch to his fork on GitHub...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin newbugfix
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and opens a pull request on GitHub to merge &lt;code&gt;bob/proj:newbugfix&lt;/code&gt; into &lt;code&gt;BrendelGroup/proj:master&lt;/code&gt;.
He then rushes out the door to go to his class.&lt;/p&gt;
&lt;h2&gt;Alice's review&lt;/h2&gt;
&lt;p&gt;When Alice returns from her lunch and seminar, she goes to GitHub to check on the status of her pull requests.
She's happy to see that Bob has accepted her second pull request, but sees that the other is still open.
She reads Bob's comments on &lt;code&gt;#14&lt;/code&gt; and after a few minutes of thinking things over herself she understands and agrees with his assessment.
She closes her unmerged pull request and deletes the &lt;code&gt;bugfix&lt;/code&gt; branch from her fork on GitHub.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin :bugfix
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Alice then takes a look at Bob's proposed solution, reviews the code, and finding it solid she merges his pull request.
GitHub automatically closes issue &lt;code&gt;#14&lt;/code&gt; at this point, since Bob's commit message included the text &lt;code&gt;Closes #14&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now Alice needs to syncronize her laptop with the new changes.
First, she switches back to the main &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At this point, her directory looks just like it did before she started work this morning.
The changes she made before lunch are stored in dedicated branches, and she has not touched the main &lt;code&gt;master&lt;/code&gt; branch on her laptop.
To update her local &lt;code&gt;master&lt;/code&gt; branch, she pulls from &lt;code&gt;BrendelGroup/proj&lt;/code&gt;...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git pull upstream master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and then pushes those updates out to her fork.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin master
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Cleanup&lt;/h2&gt;
&lt;p&gt;At this point the &lt;code&gt;master&lt;/code&gt; branch is up-to-date both on Alice's fork and her laptop.
She has also deleted the &lt;code&gt;bugfix&lt;/code&gt; branch on her fork, but the &lt;code&gt;bugfix&lt;/code&gt; branch is still present on her laptop, and the &lt;code&gt;laststep&lt;/code&gt; branch is still present both on her fork and her laptop.
She wants to delete all of these branches now, since &lt;code&gt;laststep&lt;/code&gt; has been merged and &lt;code&gt;bugfix&lt;/code&gt; is a dead end.&lt;/p&gt;
&lt;p&gt;Alice can list the branches in her local clone, highlighting the active branch, with the command...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git branch
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and she can list the branches in her GitHub fork (along with some other information) using the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git remote show origin
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Alice then uses the following commands to delete the remaining extraneous branches.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin :laststep # Delete `laststep` on `alice/proj`
git branch -d laststep    # Delete `laststep` branch on laptop
git branch -D bugfix      # Force deletion of the unmerged `bugfix` branch on laptop
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Epilogue&lt;/h2&gt;
&lt;p&gt;This story could have turned out very differently had Alice not used branches in her workflow.
She would have committed her incorrect implementation of the bug fix to her &lt;code&gt;master&lt;/code&gt; branch, followed by her correct implementation of the final step of their analysis procedure to the same &lt;code&gt;master&lt;/code&gt; branch.
Had she then submitted a pull request to merge &lt;code&gt;alice/proj:master&lt;/code&gt; into &lt;code&gt;BrendelGroup/proj:master&lt;/code&gt;, Bob would not have been able to merge it because of the erroneous bug fix.
There would be no easy way for her to keep the latest commit while discarding an earlier commit, or to integrate Bob's new solution without merge conflicts.
She likely would have had to rewind two commits in her history to get rid of her bug fix and re-implement the final step to their analysis procedure, and then use the &lt;code&gt;--force&lt;/code&gt; option to overwrite the commit history she had previously pushed to her fork.&lt;/p&gt;
&lt;p&gt;By using branches, Alice cleanly isolated each set of changes from the main development branch, and was able to discard one set of changes without affecting the other set of changes.
Using branches with git and GitHub is an excellent mechanism to keep your work organized into small manageable chunks that can be developed, evaluated, and merged independently.&lt;/p&gt;</content><category term="programming"></category><category term="git"></category></entry><entry><title>Building a decision tree chart with graphviz</title><link href="https://standage.github.io/building-a-decision-tree-chart-with-graphviz.html" rel="alternate"></link><published>2016-01-25T00:00:00-05:00</published><updated>2016-01-25T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-01-25:/building-a-decision-tree-chart-with-graphviz.html</id><summary type="html">&lt;p&gt;Walking through the process of converting my pen-and-paper sketch of a decision tree into a computer graphic with graphviz.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm preparing a manuscript, and I sketched out the following chart I would like to include.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://standage.github.io/images/ilocus-decision-tree.jpg" alt="Chart" style="width: 450px" /&gt;&lt;/p&gt;
&lt;p&gt;With my rudimentary &lt;a href="http://www.graphviz.org/"&gt;graphviz&lt;/a&gt; experience, I threw together the following representation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;digraph
{
    ilocus [label=&amp;quot;iLocus&amp;quot;]
    gilocus [label=&amp;quot;giLocus&amp;quot;]
    pilocus [label=&amp;quot;piLocus&amp;quot;]
    nilocus [label=&amp;quot;niLocus&amp;quot;]
    silocus [label=&amp;quot;siLocus&amp;quot;]
    cilocus [label=&amp;quot;ciLocus&amp;quot;]
    filocus [label=&amp;quot;fiLocus&amp;quot;]
    iilocus [label=&amp;quot;iiLocus&amp;quot;]

    containgene [label=&amp;quot;Contains gene(s)?&amp;quot;]
    proteincoding [label=&amp;quot;Protein coding?&amp;quot;]
    multiplegenes [label=&amp;quot;Multiple genes?&amp;quot;]
    geneflank [label=&amp;quot;Flanked by genes on both sides?&amp;quot;]

    ilocus -&amp;gt; containgene
    containgene -&amp;gt; geneflank [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; filocus [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; iilocus [label=&amp;quot;Yes&amp;quot;]
    containgene -&amp;gt; gilocus [label=&amp;quot;Yes&amp;quot;]
    gilocus -&amp;gt; proteincoding
    proteincoding -&amp;gt; nilocus [label=&amp;quot;No&amp;quot;]
    proteincoding -&amp;gt; pilocus [label=&amp;quot;Yes&amp;quot;]
    pilocus -&amp;gt; multiplegenes
    multiplegenes -&amp;gt; silocus [label=&amp;quot;No&amp;quot;]
    multiplegenes -&amp;gt; cilocus [label=&amp;quot;Yes&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Saved as &lt;code&gt;take1.dot&lt;/code&gt; and rendered with the command &lt;code&gt;dot -Tpng -o take1.png take.dot&lt;/code&gt;, we get this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart, take 1" src="https://standage.github.io/images/take1.png"&gt;&lt;/p&gt;
&lt;p&gt;It's not bad for a first pass, but let's adjust the style a bit to distinguish between iLocus designations and branching logic.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;digraph
{
    node [color=Limegreen,fontcolor=Limegreen,shape=oval]
    ilocus [label=&amp;quot;iLocus&amp;quot;]
    gilocus [label=&amp;quot;giLocus&amp;quot;]
    pilocus [label=&amp;quot;piLocus&amp;quot;]
    nilocus [label=&amp;quot;niLocus&amp;quot;]
    silocus [label=&amp;quot;siLocus&amp;quot;]
    cilocus [label=&amp;quot;ciLocus&amp;quot;]
    filocus [label=&amp;quot;fiLocus&amp;quot;]
    iilocus [label=&amp;quot;iiLocus&amp;quot;]

    node [color=Blue,fontcolor=Blue,shape=diamond]
    containgene [label=&amp;quot;Contains gene(s)?&amp;quot;]
    proteincoding [label=&amp;quot;Protein coding?&amp;quot;]
    multiplegenes [label=&amp;quot;Multiple genes?&amp;quot;]
    geneflank [label=&amp;quot;Flanked by genes\non both sides?&amp;quot;]

    ilocus -&amp;gt; containgene
    containgene -&amp;gt; geneflank [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; filocus [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; iilocus [label=&amp;quot;Yes&amp;quot;]
    containgene -&amp;gt; gilocus [label=&amp;quot;Yes&amp;quot;]
    gilocus -&amp;gt; proteincoding
    proteincoding -&amp;gt; nilocus [label=&amp;quot;No&amp;quot;]
    proteincoding -&amp;gt; pilocus [label=&amp;quot;Yes&amp;quot;]
    pilocus -&amp;gt; multiplegenes
    multiplegenes -&amp;gt; silocus [label=&amp;quot;No&amp;quot;]
    multiplegenes -&amp;gt; cilocus [label=&amp;quot;Yes&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us the following.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart, take 2" src="https://standage.github.io/images/take2.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, let's see if we can get the &lt;code&gt;giLocus&lt;/code&gt; and &lt;code&gt;piLocus&lt;/code&gt; nodes on the same level as in the sketch, using &lt;code&gt;rank=same&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;digraph
{
    node [color=Limegreen,fontcolor=Limegreen,shape=oval]
    ilocus [label=&amp;quot;iLocus&amp;quot;]
    gilocus [label=&amp;quot;giLocus&amp;quot;]
    pilocus [label=&amp;quot;piLocus&amp;quot;]
    nilocus [label=&amp;quot;niLocus&amp;quot;]
    silocus [label=&amp;quot;siLocus&amp;quot;]
    cilocus [label=&amp;quot;ciLocus&amp;quot;]
    filocus [label=&amp;quot;fiLocus&amp;quot;]
    iilocus [label=&amp;quot;iiLocus&amp;quot;]

    node [color=Blue,fontcolor=Blue,shape=diamond]
    containgene [label=&amp;quot;Contains gene(s)?&amp;quot;]
    proteincoding [label=&amp;quot;Protein coding?&amp;quot;]
    multiplegenes [label=&amp;quot;Multiple genes?&amp;quot;]
    geneflank [label=&amp;quot;Flanked by genes\non both sides?&amp;quot;]

    {rank = same; containgene; gilocus}
    {rank = same; proteincoding; pilocus}

    ilocus -&amp;gt; containgene
    containgene -&amp;gt; geneflank [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; filocus [label=&amp;quot;No&amp;quot;]
    geneflank -&amp;gt; iilocus [label=&amp;quot;Yes&amp;quot;]
    containgene -&amp;gt; gilocus [label=&amp;quot;Yes&amp;quot;]
    gilocus -&amp;gt; proteincoding
    proteincoding -&amp;gt; nilocus [label=&amp;quot;No&amp;quot;]
    proteincoding -&amp;gt; pilocus [label=&amp;quot;Yes&amp;quot;]
    pilocus -&amp;gt; multiplegenes
    multiplegenes -&amp;gt; silocus [label=&amp;quot;No&amp;quot;]
    multiplegenes -&amp;gt; cilocus [label=&amp;quot;Yes&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That gives us the following.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart, take 3" src="https://standage.github.io/images/take3.png"&gt;&lt;/p&gt;
&lt;p&gt;Getting there!
Now, let's use the &lt;code&gt;compass_pt&lt;/code&gt; feature to specify which side of the diamonds we want the arrows to come from.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;digraph
{
    node [color=Limegreen,fontcolor=Limegreen,shape=oval]
    ilocus [label=&amp;quot;iLocus&amp;quot;]
    gilocus [label=&amp;quot;giLocus&amp;quot;]
    pilocus [label=&amp;quot;piLocus&amp;quot;]
    nilocus [label=&amp;quot;niLocus&amp;quot;]
    silocus [label=&amp;quot;siLocus&amp;quot;]
    cilocus [label=&amp;quot;ciLocus&amp;quot;]
    filocus [label=&amp;quot;fiLocus&amp;quot;]
    iilocus [label=&amp;quot;iiLocus&amp;quot;]

    node [color=Blue,fontcolor=Blue,shape=diamond]
    containgene [label=&amp;quot;Contains gene(s)?&amp;quot;]
    proteincoding [label=&amp;quot;Protein coding?&amp;quot;]
    multiplegenes [label=&amp;quot;Multiple genes?&amp;quot;]
    geneflank [label=&amp;quot;Flanked by genes\non both sides?&amp;quot;]

    {rank = same; containgene; gilocus}
    {rank = same; proteincoding; pilocus}

    ilocus -&amp;gt; containgene
    containgene:w -&amp;gt; geneflank [label=&amp;quot;No&amp;quot;]
    geneflank:e -&amp;gt; filocus [label=&amp;quot;No&amp;quot;]
    geneflank:w -&amp;gt; iilocus [label=&amp;quot;Yes&amp;quot;]
    containgene:e -&amp;gt; gilocus [label=&amp;quot;Yes&amp;quot;]
    gilocus -&amp;gt; proteincoding
    proteincoding:w -&amp;gt; nilocus [label=&amp;quot;No&amp;quot;]
    proteincoding:e -&amp;gt; pilocus [label=&amp;quot;Yes&amp;quot;]
    pilocus -&amp;gt; multiplegenes
    multiplegenes:e -&amp;gt; silocus [label=&amp;quot;No&amp;quot;]
    multiplegenes:w -&amp;gt; cilocus [label=&amp;quot;Yes&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We end up with this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart, take 4" src="https://standage.github.io/images/take4.png"&gt;&lt;/p&gt;
&lt;p&gt;Everything looks good here except for the edges.
Is there any way we can force straight edges with corners, like in the sketch, rather than the curved edges?
The &lt;code&gt;splines=ortho&lt;/code&gt; setting seems to be designed just for that.
Let's try it out (also requires setting edge labels to &lt;code&gt;xlabel&lt;/code&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;digraph
{
    graph [splines=ortho]

    node [color=Limegreen,fontcolor=Limegreen,shape=oval]
    ilocus [label=&amp;quot;iLocus&amp;quot;]
    gilocus [label=&amp;quot;giLocus&amp;quot;]
    pilocus [label=&amp;quot;piLocus&amp;quot;]
    nilocus [label=&amp;quot;niLocus&amp;quot;]
    silocus [label=&amp;quot;siLocus&amp;quot;]
    cilocus [label=&amp;quot;ciLocus&amp;quot;]
    filocus [label=&amp;quot;fiLocus&amp;quot;]
    iilocus [label=&amp;quot;iiLocus&amp;quot;]

    node [color=Blue,fontcolor=Blue,shape=diamond]
    containgene [label=&amp;quot;Contains gene(s)?&amp;quot;]
    proteincoding [label=&amp;quot;Protein coding?&amp;quot;]
    multiplegenes [label=&amp;quot;Multiple genes?&amp;quot;]
    geneflank [label=&amp;quot;Flanked by genes\non both sides?&amp;quot;]

    {rank = same; containgene; gilocus}
    {rank = same; proteincoding; pilocus}

    ilocus -&amp;gt; containgene
    containgene:w -&amp;gt; geneflank [xlabel=&amp;quot;No&amp;quot;]
    geneflank:e -&amp;gt; filocus [xlabel=&amp;quot;No&amp;quot;]
    geneflank:w -&amp;gt; iilocus [xlabel=&amp;quot;Yes&amp;quot;]
    containgene:e -&amp;gt; gilocus [xlabel=&amp;quot;Yes&amp;quot;]
    gilocus -&amp;gt; proteincoding
    proteincoding:w -&amp;gt; nilocus [xlabel=&amp;quot;No&amp;quot;]
    proteincoding:e -&amp;gt; pilocus [xlabel=&amp;quot;Yes&amp;quot;]
    pilocus -&amp;gt; multiplegenes
    multiplegenes:e -&amp;gt; silocus [xlabel=&amp;quot;No&amp;quot;]
    multiplegenes:w -&amp;gt; cilocus [xlabel=&amp;quot;Yes&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which gives us this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart, take 5" src="https://standage.github.io/images/take5.png"&gt;&lt;/p&gt;
&lt;p&gt;Whoa, definitely not what I had in mind.
I haven't been able to figure this last step out.
If I can't figure out how to force straight edges with corners, then I think I'll have to go with "take 3" above, the last figure before the &lt;code&gt;compass_pt&lt;/code&gt; feature was used.&lt;/p&gt;</content><category term="visualization"></category><category term="graphviz"></category></entry><entry><title>Formatting C code with clang-format</title><link href="https://standage.github.io/formatting-c-code-with-clang-format.html" rel="alternate"></link><published>2016-01-19T00:00:00-05:00</published><updated>2016-01-19T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-01-19:/formatting-c-code-with-clang-format.html</id><summary type="html">&lt;p&gt;Found a nifty tool for enforcing formatting styles for C code.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Code formatting styles: the thing we all love to hate.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The rules are like guidelines anyway" src="https://standage.github.io/images/pirate-guidelines.gif"&gt;&lt;/p&gt;
&lt;p&gt;The best formatting style is the style you don't have to think about, the style that helps you focus on the logical flow of your code.
The best formatting style is consistent and organic, providing an intuitive visual representation of program organization.&lt;/p&gt;
&lt;p&gt;On the other hand, pedantic enforcement formatting styles can outweigh any benefits you get from it in the first place.&lt;/p&gt;
&lt;p&gt;A lot of digital ink has been spilled and flame wars fought over this topic, and I will not belabor the point here.
Suffice it to say that I think that enforcement of code formatting style is, on the whole, a good thing.
Styles focus mostly on readability, which is a HUGE DEAL when it comes to collaborative development and maintenance of code.
Simplifying the &lt;em&gt;maintenance&lt;/em&gt; of your code will save much more of your time and others' time in the future than almost any technical optimization.
Explicitly enforcing a style convention (like Python's PEP8, and associated tools &lt;code&gt;pep8&lt;/code&gt; and &lt;code&gt;autopep8&lt;/code&gt;) minimize conflicts between project contributors, and make it easier to integrate contributions from new contributors.
Exceptions can be considered on a per-case basis, and most enforcement tools provide mechanisms for marking code to ignore, but for the most part the rules are there to be followed for everyone's benefit.&lt;/p&gt;
&lt;p&gt;For Python programming, the PEP8 style is the uncontested standard.
There may be others out there, but they're surely obscure and &lt;em&gt;ad hoc&lt;/em&gt;.
When it comes to C and C++, however, there are many different popular styles.
There are lots of commonalities, but the differences (particularly in the placement of braces) make a huge difference in the appearance of the code.
I prefer the popular Allman style, the K&amp;amp;R style (also popular) is pretty reasonable, but styles like Whitesmith and Pico are just nuts!
See &lt;a href="http://www.terminally-incoherent.com/blog/2009/04/10/the-only-correct-indent-style/"&gt;this page&lt;/a&gt; for some examples.&lt;/p&gt;
&lt;p&gt;So in some &lt;a href="https://matt.sh/howto-c"&gt;recent reading&lt;/a&gt; I came across a tool called &lt;code&gt;clang-format&lt;/code&gt; that will automatically format C code based on your preferred style.
All of my C code formatting has previously been self-enforced, but after using &lt;code&gt;pep8&lt;/code&gt; and &lt;code&gt;autopep8&lt;/code&gt; on my Python code for a few years now I thought it was time to let an automated tool do the work for me.
Here's my config file (&lt;code&gt;.clang-format&lt;/code&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;BasedOnStyle&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;llvm&lt;/span&gt;
&lt;span class="n"&gt;IndentWidth&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;AllowShortFunctionsOnASingleLine&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;None&lt;/span&gt;
&lt;span class="n"&gt;KeepEmptyLinesAtTheStartOfBlocks&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="n"&gt;BreakBeforeBraces&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Allman&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And here's the command I use to reformat a file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clang-format -i -style=file src/locuspocus.c
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Credits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matt.sh/howto-c"&gt;This post&lt;/a&gt; got me going on this.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/29477654/how-to-make-clang-format-add-new-line-before-opening-brace-of-a-function"&gt;This thread&lt;/a&gt; was helpful&lt;/li&gt;
&lt;li&gt;&lt;a href="http://clang.llvm.org/docs/ClangFormatStyleOptions.html"&gt;This page&lt;/a&gt; has the official documentation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/25880990/clang-format-breaks-lint-annotations"&gt;This answer&lt;/a&gt; describes a nice trick for disabling style checks for a code block.&lt;/li&gt;
&lt;/ul&gt;</content><category term="programming"></category><category term="c"></category></entry><entry><title>PEP8 and the Atom editor</title><link href="https://standage.github.io/pep8-and-the-atom-editor.html" rel="alternate"></link><published>2016-01-19T00:00:00-05:00</published><updated>2016-01-19T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2016-01-19:/pep8-and-the-atom-editor.html</id><summary type="html">&lt;p&gt;I tried out the &lt;a href="https://atom.io/"&gt;Atom text editor&lt;/a&gt; very soon after it came out.
For some reason--whether a half-hearted attempt on my part, or the fact that Atom was an unrefined beta--I didn't love it.
After only a day or two, I reverted to the Komodo Edit editor I had been comfortable with for several years.&lt;/p&gt;
&lt;p&gt;Recently, I heard a colleague â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I tried out the &lt;a href="https://atom.io/"&gt;Atom text editor&lt;/a&gt; very soon after it came out.
For some reason--whether a half-hearted attempt on my part, or the fact that Atom was an unrefined beta--I didn't love it.
After only a day or two, I reverted to the Komodo Edit editor I had been comfortable with for several years.&lt;/p&gt;
&lt;p&gt;Recently, I heard a colleague describe Atom as a "Markdown editor".
I was surprised, since my impression is that Atom was developed as a general purpose editor.
But after my colleague showed me Atom's real-time Markdown render/preview feature, I figured I might give Atom a try again.&lt;/p&gt;
&lt;p&gt;I'm enjoying Atom much better this time around.
After working with it for a few days doing some basic programming (Python) and writing (Markdown) I had no complaints.
It was only when I started revisiting some of my C code that I started having problems.&lt;/p&gt;
&lt;p&gt;Not serious problems by any means: maybe &lt;em&gt;annoyances&lt;/em&gt; is a better word.
Anyway, I follow PEP8 for the most part when I write Python code, so I had my wrap guide set to 79 characters.
This became awkward when I am working on C code, where I have long used 80 characters as the limit.
I was not enthusiastic about switching the wrap guide every time I changed langauges, which seemed to be the only alternative to just switching the whole thing off.&lt;/p&gt;
&lt;p&gt;Since Atom is touted as the ultimate hackable and configurable editor, I figured I would see if the wrap guide is something I can configure on a per-language basis.
Surely enough, a bit of Google searching uncovered &lt;a href="https://atom.io/packages/wrap-guide"&gt;the secret&lt;/a&gt;.
The &lt;code&gt;preferredLineLength&lt;/code&gt; attribute under &lt;code&gt;editor&lt;/code&gt; controls where the wrap guide is displayed.
I set the default (under &lt;code&gt;*&lt;/code&gt;) to 80, and then I created another Python-specific configuration with &lt;code&gt;".source.py"&lt;/code&gt;, with &lt;code&gt;preferredLineLength&lt;/code&gt; set to 79.&lt;/p&gt;
&lt;p&gt;Here is my Atom configuration file, stored at &lt;code&gt;~/.atom/config.cson&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot;*&amp;quot;:
  core:
    disabledPackages: [
      &amp;quot;spell-check&amp;quot;
    ]
    projectHome: &amp;quot;/Users/standage/Software&amp;quot;
  editor:
    fontFamily: &amp;quot;Consolas&amp;quot;
    invisibles: {}
    preferredLineLength: 80
    showIndentGuide: true
    tabLength: 4
  welcome:
    showOnStartup: false

&amp;quot;.source.py&amp;quot;:
  &amp;quot;editor:&amp;quot;:
    preferredLineLength: 79
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's it!&lt;/p&gt;</content><category term="programming"></category></entry><entry><title>Misconceptions about research software</title><link href="https://standage.github.io/misconceptions-about-research-software.html" rel="alternate"></link><published>2015-12-16T00:00:00-05:00</published><updated>2015-12-16T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-12-16:/misconceptions-about-research-software.html</id><summary type="html">&lt;p&gt;I stumbled across &lt;a href="https://www.quora.com/I-have-5-years-of-working-experience-but-I-still-code-very-slow-How-can-I-code-faster-What-should-I-learn"&gt;the following question&lt;/a&gt; the other day in an email digest from Quora.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have 5 years of working experience, but I still code very slow.
How can I code faster? What should I learn?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most of the responses don't answer the question directly, but instead make the point that speed is a very poor metric by which â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I stumbled across &lt;a href="https://www.quora.com/I-have-5-years-of-working-experience-but-I-still-code-very-slow-How-can-I-code-faster-What-should-I-learn"&gt;the following question&lt;/a&gt; the other day in an email digest from Quora.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have 5 years of working experience, but I still code very slow.
How can I code faster? What should I learn?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most of the responses don't answer the question directly, but instead make the point that speed is a very poor metric by which to measure quality or productivity in software engineering.
Carefully designing, documenting, implementing, and testing code requires considerable time and resources upfront, but not nearly as much time and resources as is required in later stages to maintain or fix code that was written quickly and sloppily.&lt;/p&gt;
&lt;p&gt;In this case, I think the &lt;a href="http://meta.stackexchange.com/a/146514/151867"&gt;OP&lt;/a&gt; fell into the trap of thinking that writing code is just about writing code.
In reality, code is just a projection of a programmer's conceptual model of something onto a representation that a computer can interpret and execute.
If your initial conceptual model is clear, your code will be clear and easy to write.
If your initial conceptual model is fuzzy, you'll probably end up taking the long way to a workable solution, leaving lots of crufty code in your wake.
Research software engineering is so much more than just "writing code".&lt;/p&gt;
&lt;p&gt;In the ideal situation (i.e., with a clear and accurate mathematical model in hand, and with informed choices about which algorithms and data structures provide an efficient computational solution), writing the actual code is almost an afterthought.
Once the model is nailed down, the programmer has quite a bit of technical freedom when it comes to implementation and testing, but the precise details of implementation are of little interest compared to the results produced by the software.&lt;/p&gt;
&lt;p&gt;In the real world, I get the impression that most programmers and programmer-scientists rarely achieve this ideal, and end up developing and refining their models &lt;em&gt;as they write the code&lt;/em&gt;.
I think some of this has to do with lack of discipline, which each scientist has to address on a personal level.
I think it also has to do with lack of training, of which I think we could be doing much better job as a field.&lt;/p&gt;
&lt;p&gt;But I think the notion (all too common in academics) that research software engineering is a technical exercise ("just writing code") rather than an intellectual one is particularly problematic.
It's problematic for those that create the software, who all too often jump head-first into &lt;em&gt;writing code&lt;/em&gt; without having done the proper modeling and design upfront.
It's problematic for those in a position to supervise or evaluate or fund research, who all too often trivialize software because it's "just writing code".&lt;/p&gt;
&lt;p&gt;A few months ago the bioinformatics twittersphere/blogosphere got caught up in a great discussion on the purpose of software in academic research.
I was able to dig through some old notes and find a link to &lt;a href="http://phdops.kblin.org/software-dev-intellectual-contribution.html"&gt;an excellent blog post&lt;/a&gt; which provides links to lots of good reading on the topic.
I particularly like the analogy he made between experiments (design vs execution) and software (design vs implementation).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Software development in science is often compared to conductiong [sic] experimental work, and because most people argue that experimental work is a means to an end, that is not a primary product of science.
So, by analogy, neither should software development be.
The actual &lt;em&gt;scientific&lt;/em&gt; contribution is designing the experiment in the first place.
Now, the analogy has one slight problem here, and I'm tempted to blame that problem on the pretty shoddy practices around software development in science, people have written about this before.
Mainly, people seem to assume that &lt;strong&gt;implementing&lt;/strong&gt; the software is all there is for software development.
I humbly disagree.
I would argue that the analogy with wet lab work would be that &lt;strong&gt;designing experiments&lt;/strong&gt; is like &lt;strong&gt;designing software&lt;/strong&gt;, and &lt;strong&gt;conducting experiments&lt;/strong&gt; is like &lt;strong&gt;implementing software&lt;/strong&gt;.
Now if we follow the general agreement that designing experiments is a valuable intellectual input to the scientific endeavour, by analogy so should designing software.
So, the answer the question I asked in the topic of the post "Is software development in science an intellectual contribution?" would be "Yes, parts of it at least".&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I agree: the design of research software should be treated as a valid intellectual contribution in the academic science enterprise, and credit should be rendered accordingly.&lt;/p&gt;</content><category term="software"></category><category term="academics"></category><category term="credit"></category></entry><entry><title>Shell pipelines in Python</title><link href="https://standage.github.io/shell-pipelines-in-python.html" rel="alternate"></link><published>2015-12-15T00:00:00-05:00</published><updated>2015-12-15T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-12-15:/shell-pipelines-in-python.html</id><summary type="html">&lt;p&gt;The UNIX shell is an indispensible tool for project organization and data management in bioinformatics.
I spend &lt;em&gt;a lot&lt;/em&gt; of time in the shell, and having picked up on a lot of time-saving techniques over the years it might just be my favorite computing environment.&lt;/p&gt;
&lt;p&gt;The shell has its limitations, however.
Piping, the very feature that gives the shell its â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The UNIX shell is an indispensible tool for project organization and data management in bioinformatics.
I spend &lt;em&gt;a lot&lt;/em&gt; of time in the shell, and having picked up on a lot of time-saving techniques over the years it might just be my favorite computing environment.&lt;/p&gt;
&lt;p&gt;The shell has its limitations, however.
Piping, the very feature that gives the shell its amazing power and flexibility, can also lead to some pretty gruesome syntax.
Debugging shell code is tough, the error handling is rudimentary, and good luck finding a good framework for automated tests.
In short, the shell is great for interactive computing and automating simple tasks, but when it comes to workflows requiring more fine-grained control, a language like Python is often a better choice.&lt;/p&gt;
&lt;p&gt;Here I provide a Python translation of several shell commands.&lt;/p&gt;
&lt;h2&gt;Simplest case&lt;/h2&gt;
&lt;p&gt;You don't typically get much bioinformatics work done with a single command without arguments.
Anything substantial will involve data files, parameters, and so on, that are typically specified using arguments on the command line (you don't have these hard coded in a script, do you?!?!).
But just for sake of completeness, it's very straightforward to execute shell commands this way in Python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Python equivalent is as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ls&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There is even the simpler &lt;code&gt;call&lt;/code&gt; function...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ls&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...but for most situations I find the &lt;code&gt;check_call&lt;/code&gt; more useful, since it will halt the Python code immediately if the subprocess returns a non-zero status.&lt;/p&gt;
&lt;h2&gt;Command with arguments&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls -lhp
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Commands with arguments cannot simply be dropped in to the &lt;code&gt;check_call&lt;/code&gt; command as-is.
The following code will fail.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ls -lhp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are two ways you can fix this: the convenient (and wrong and insecure) way...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ls -lhp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and The Right Way.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ls&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-lhp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The convenience of the first method comes at the cost of security: the &lt;code&gt;shell=True&lt;/code&gt; setting introduces vulnerability to &lt;a href="https://security.openstack.org/guidelines/dg_use-subprocess-securely.html"&gt;shell injections&lt;/a&gt;.
This isn't the type of thing you expect to encounter much in the research setting, but it's an important consideration nonetheless, and exceptions should be made with caution.&lt;/p&gt;
&lt;p&gt;This example is pretty silly, since you'll probably never need to call the &lt;code&gt;ls&lt;/code&gt; command from Python.
Let's do a different example you're much more likely to encounter in bioinformatics.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;blastx -db /opt/ncbi/nr -query tsa.fasta &lt;span class="se"&gt;\&lt;/span&gt;
       -evalue 1e-4 -num_threads &lt;span class="nv"&gt;$numthreads&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
       -out tsa-vs-nr.blastx
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Python equivalent is as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blastx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-db&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/opt/ncbi/nr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-query&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tsa.fasta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="s1"&gt;&amp;#39;-evalue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;1e-4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-num_threads&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numthreads&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="s1"&gt;&amp;#39;-out&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tsa-vs-nr.blastx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Redirect stdin and stdout&lt;/h2&gt;
&lt;p&gt;Many programs and commands allow you to specify input and output files as arguments, as in the &lt;code&gt;blastx&lt;/code&gt; command above.
However, sometimes your only options are &lt;code&gt;stdin&lt;/code&gt; and/or &lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sed s/^scaffold_/PcanScaf/ &amp;lt; pcan-in.gff3 &amp;gt; pcan-out.gff3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is the Python equivalent.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pcan-in.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;instream&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pcan-out.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;outstream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_call&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s/^scaffold_/PcanScaf/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;instream&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;outstream&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;The main event: pipelines&lt;/h2&gt;
&lt;p&gt;Handling input and output for single commands is great and all, but the real power of the shell is piping commands together like so.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;grep -v &lt;span class="s1"&gt;$&amp;#39;\tintron\t&amp;#39;&lt;/span&gt; loci.gff3 &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; pmrna --locus --accession --map&lt;span class="o"&gt;=&lt;/span&gt;map.txt &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; canon-gff3 --outfile&lt;span class="o"&gt;=&lt;/span&gt;locus.mrnas.gff3 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Unless we want to introduce security vulnerabilities, we cannot simply run these commands with a single call to the &lt;code&gt;check_call&lt;/code&gt; function.
For this use case, we want to use the &lt;code&gt;Popen&lt;/code&gt; constructor and the &lt;code&gt;communicate&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grepproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;grep&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-v&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;intron&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;loci.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pmrnaproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pmrna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--locus&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--accession&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--map=map.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;grepproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;canonproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;canon-gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--outfile=locus.mrnas.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pmrnaproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;canonproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;communicate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If needed, it is trivial to capture the terminal output like so.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grepproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;grep&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-v&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;intron&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;loci.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pmrnaproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pmrna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--locus&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--accession&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--map=map.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;grepproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;canonproc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;canon-gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--outfile=locus.mrnas.gff3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pmrnaproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PIPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stderr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canonproc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;communicate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# process the terminal warnings&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Coda&lt;/h2&gt;
&lt;p&gt;That's it!&lt;/p&gt;
&lt;p&gt;Python's &lt;code&gt;subprocess&lt;/code&gt; module is pretty powerful, and allows even slicker interactions with the shell, such as printing text to a pipeline of shell commands.
However, writing to and reading from a pipeline simultaneously can get tricky and is prone to deadlocks.
I will not cover this here, but the Internet is full of blog posts and StackOverflow threads discussing the intricacies of the &lt;code&gt;subprocess&lt;/code&gt; for these more complicated use cases.&lt;/p&gt;</content><category term="shell"></category><category term="python"></category></entry><entry><title>Filter stderr while piping in UNIX</title><link href="https://standage.github.io/filter-stderr-while-piping-in-unix.html" rel="alternate"></link><published>2015-12-14T00:00:00-05:00</published><updated>2015-12-14T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-12-14:/filter-stderr-while-piping-in-unix.html</id><summary type="html">&lt;p&gt;Nifty trick for filtering stderr in the middle of a shell pipeline.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've written before about &lt;a href="https://biowize.wordpress.com/2013/10/11/commands-in-place-of-program-arguments/"&gt;process substitutions in the shell&lt;/a&gt;.
This has become a core technique I use almost on a daily basis in my data work in the shell.
It has many uses, but I want to highlight a particular one here.&lt;/p&gt;
&lt;h2&gt;Use case&lt;/h2&gt;
&lt;p&gt;Imagine you have a shell script with the following commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;first_command --arg1&lt;span class="o"&gt;=&lt;/span&gt;foo --arg2&lt;span class="o"&gt;=&lt;/span&gt;bar --flag3 infile.txt &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; second_command one two three &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; third_command --abc xyz &lt;span class="se"&gt;\&lt;/span&gt;
    &amp;gt; outfile.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Each of these commands will print to stderr in case of a warning or an error.
However, &lt;code&gt;second_command&lt;/code&gt; prints an irrelevant warning (&lt;code&gt;cannot find "foo"&lt;/code&gt;) message over and over again, filling up the terminal with thousands of lines of noise and making it more difficult to find warnings or error messages we might actually care about.
How can we filter the stderr of &lt;code&gt;second_command&lt;/code&gt; so that the &lt;code&gt;cannot find "foo"&lt;/code&gt; messages are ignored, but all other messages still show up?&lt;/p&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;Process substitutions can be used not only as pseudo input files using the &lt;code&gt;&amp;lt;()&lt;/code&gt; syntax, but also as pseudo output files using the &lt;code&gt;&amp;gt;()&lt;/code&gt; syntax.
If we redirect a program's output to a process, we can then filter the data within that process, like so.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;some_program &amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;sort &lt;span class="p"&gt;|&lt;/span&gt; uniq -c&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Extending this to stderr requires only two changes.
First, we replace &lt;code&gt;&amp;gt;&lt;/code&gt; with &lt;code&gt;2&amp;gt;&lt;/code&gt; so that we are redirecting the correct output stream.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;some_program &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;sort &lt;span class="p"&gt;|&lt;/span&gt; uniq -c&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Secondly, we add &lt;code&gt;1&amp;gt;&amp;amp;2&lt;/code&gt; to the end of the process so that its stdout is redirected back to stderr, which is where the data was intended to go in the first place (cue Ghostbusters quote about not crossing the streams).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;some_program &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;sort &lt;span class="p"&gt;|&lt;/span&gt; uniq -c &lt;span class="m"&gt;1&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Putting this all together and going back to our original use case, we can remove the unwanted &lt;code&gt;cannot find "foo"&lt;/code&gt; messages from our terminal like so.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;first_command --arg1&lt;span class="o"&gt;=&lt;/span&gt;foo --arg2&lt;span class="o"&gt;=&lt;/span&gt;bar --flag3 infile.txt &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; second_command one two three &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; &amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;grep &lt;span class="s1"&gt;&amp;#39;cannot find &amp;quot;foo&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="p"&gt;|&lt;/span&gt; third_command --abc xyz &lt;span class="se"&gt;\&lt;/span&gt;
    &amp;gt; outfile.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's it!&lt;/p&gt;</content><category term="shell"></category></entry><entry><title>Consortium authors in EndNote</title><link href="https://standage.github.io/consortium-authors-in-endnote.html" rel="alternate"></link><published>2015-11-21T00:00:00-05:00</published><updated>2015-11-21T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-11-21:/consortium-authors-in-endnote.html</id><summary type="html">&lt;p&gt;Today I was reformatting a manuscript and changing from a numbered citation style to an author/date citation style.
In doing so, I found some issues with my EndNote references library that had gone unnoticed before since they were tucked away in the reference list.
One of the issues involved citation of a paper with a single consortium author, designated â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I was reformatting a manuscript and changing from a numbered citation style to an author/date citation style.
In doing so, I found some issues with my EndNote references library that had gone unnoticed before since they were tucked away in the reference list.
One of the issues involved citation of a paper with a single consortium author, designated in the library as &lt;strong&gt;Honeybee_Genome_Sequencing_Consortium&lt;/strong&gt;.
I wasn't clear why it was formatted this way until I tried to fix the author field, and the citation then became &lt;strong&gt;Consortium HGS&lt;/strong&gt;, as if this was a person's name.
Neither the underscores nor the initials were satisfactory, so I decided to see if the Internet's collective wisdom (i.e. Google) had any light to shed on the issue.&lt;/p&gt;
&lt;p&gt;It turns out that one way (the way?) to solve this issue is to type the consortium name as you want it to appear, and then append a comma at the end (in my case, &lt;strong&gt;Honeybee Genome Sequencing Consortium,&lt;/strong&gt;).
This tricks EndNote into thinking this is a surname, and thus it does not attempt to initialize the name.
Perhaps there's a cleaner way, but this worked for me!&lt;/p&gt;
&lt;p&gt;Hat tip to the University of Warwick for &lt;a href="http://blogs.warwick.ac.uk/endnoteweb/entry/entering_names_of/"&gt;this blog post&lt;/a&gt;!&lt;/p&gt;</content><category term="endnote"></category><category term="formatting"></category></entry><entry><title>New lab notebook setup: my motivation</title><link href="https://standage.github.io/new-lab-notebook-setup-my-motivation.html" rel="alternate"></link><published>2015-11-17T00:00:00-05:00</published><updated>2015-11-17T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-11-17:/new-lab-notebook-setup-my-motivation.html</id><summary type="html">&lt;p&gt;My first research blog was a self-hosted Wordpress blog run from an Ubuntu server in our lab.
I was initially enamored by the supposed control and flexibility this gave me, but when I changed institutions a year or so later I was not quite as enamored with the work that went into migrating the content, nor the time I spent â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;My first research blog was a self-hosted Wordpress blog run from an Ubuntu server in our lab.
I was initially enamored by the supposed control and flexibility this gave me, but when I changed institutions a year or so later I was not quite as enamored with the work that went into migrating the content, nor the time I spent in sysadmin troubleshooting (I hate SELinux with a passion).
During that time I had also become familiar with the disheartening statistics on the half-life of links to academic department/lab websites.
I so decided that in the long run it would be more sustainable for me to let someone else handle the hosting concerns, so I could focus just on the content.
Version 2 of my blog was (well, is) &lt;a href="http://biowize.wordpress.com"&gt;hosted at Wordpress.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've continued posting content to the blog over the last couple of yearsâ€”though not as frequently as I would likeâ€”but recently I've become increasingly dissatisfied.
I found myself saving electronic "scribbles" in Evernote or my wiki (also self-hosted on a server in our lab) and IPython/Jupyter notebooks in GitHub gists, saving the blog for when I really wanted to stretch my writing legs and produce some polished, contextualized thoughts.
But with notes and code and blog posts scattered all over the place, my setup felt harried and scrambled.
I need a more organized and streamlined solution.&lt;/p&gt;
&lt;p&gt;In working out the setup for this notebook, I have the following considerations in mind.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;One-stop shop&lt;/strong&gt;. With the exception of my software engineering work (which benefits immensely from the development workflow provided by git and GitHub), I want to be able to store all of my notes, scribbles, code snippets, Jupyter notebooks, and blog posts in one convenient location.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;. I was never a fan of web-based WYSIWYG text editors, and so I've been writing all of my blog content in raw HTML.
  It's a travesty that this is still happening in 2015.
  I would much prefer to format my content with Markdown.
  (Ironically, I just learned recently that Wordpress.com now has integrated support for Markdown, so this would make a poor excuse if it was my only reason for changing.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embedded JavaScript&lt;/strong&gt;. Wordpress.com does not allow users to embed &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tags in blog posts.
  "Shortcodes" are available for embedding content from a variety of popular media sources (Youtube, GitHub gist, and Twitter, for example), but you're out of luck if you want to embed video or other rich content from a different source (such as an &lt;a href="https://asciinema.org/"&gt;asciicast&lt;/a&gt;).
  Support for embedded JavaScript is high on my priority list for this new setup.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control and accessibility&lt;/strong&gt;. The first self-hosted version of the blog gave me the illusion of control and accessibility with respect to content, but really I was stuck to the web authoring interface (behind the scenes it was raw HTML stored in a MySQL database, and I wanted nothing to do with that bidness).
  The second version of my blog relieved me of the tedium of hosting, but only by relinquishing even more accessibility and control over my content.
  For my new setup, I would like the ability to write my content in vim or Komodo Edit; to search the content with shell tools or with custom Python scripts; to set up automated backup procedures using cron.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Taking all this into account, it's pretty clear that I need a solution based on a &lt;a href="https://wiki.python.org/moin/StaticSiteGenerator"&gt;static site generator&lt;/a&gt;.&lt;/p&gt;</content><category term="meta"></category></entry><entry><title>Hello, world!</title><link href="https://standage.github.io/hello-world.html" rel="alternate"></link><published>2015-11-16T00:00:00-05:00</published><updated>2015-11-16T00:00:00-05:00</updated><author><name>Daniel S. Standage</name></author><id>tag:standage.github.io,2015-11-16:/hello-world.html</id><summary type="html">&lt;p&gt;Seed content for my new notebook.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Seed content for my new notebook.&lt;/p&gt;</content><category term="meta"></category></entry></feed>